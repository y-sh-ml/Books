{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beca3f23",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005741,
     "end_time": "2022-06-01T05:41:22.715773",
     "exception": false,
     "start_time": "2022-06-01T05:41:22.710032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7.5　CNN の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595f23ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T05:41:22.730774Z",
     "iopub.status.busy": "2022-06-01T05:41:22.729790Z",
     "iopub.status.idle": "2022-06-01T05:41:22.745433Z",
     "shell.execute_reply": "2022-06-01T05:41:22.744186Z"
    },
    "papermill": {
     "duration": 0.024976,
     "end_time": "2022-06-01T05:41:22.748380",
     "exception": false,
     "start_time": "2022-06-01T05:41:22.723404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26a56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/yuta.shimizu/Downloads/Machine Learning/deep-learning-from-scratch-master/ch07')\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e7547",
   "metadata": {},
   "source": [
    "### 7.5.1　CNN の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7143ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(\n",
    "        self, input_dim=(1, 28, 28),\n",
    "        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "        hidden_size=100,\n",
    "        out_size=10,\n",
    "        weight_init_std=0.01\n",
    "    ):\n",
    "        \n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        def predict(self, x):\n",
    "            for layer in self.layers.values():\n",
    "                x = layer.forward(x)\n",
    "            return x\n",
    "        \n",
    "        def loss(self, x, t):\n",
    "            y = self.predict(x)\n",
    "            return self.last_layer.forward(y, t)\n",
    "        \n",
    "        def gradient(self, x, t):\n",
    "            self.loss(x, t)\n",
    "            \n",
    "            dout = 1\n",
    "            dout = self.last_layer.backward(dout)\n",
    "            \n",
    "            layers = list(self.layers.values())\n",
    "            layers.reverse()\n",
    "            for layer in layers:\n",
    "                dout = layer.backward(dout)\n",
    "                \n",
    "            grads = {}\n",
    "            grads['W1'] = self.layers['Conv1'].dW\n",
    "            grads['b1'] = self.layers['Conv1'].db\n",
    "            grads['W2'] = self.layers['Affine1'].dW\n",
    "            grads['b2'] = self.layers['Affine1'].db\n",
    "            grads['W3'] = self.layers['Affine2'].dW\n",
    "            grads['b3'] = self.layers['Affine2'].db\n",
    "            \n",
    "            return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4ba38",
   "metadata": {},
   "source": [
    "### 7.5.2　CNN の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a4b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160b9a26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300116532724218\n",
      "=== epoch:1, train acc:0.265, test acc:0.292 ===\n",
      "train loss:2.2961534723318127\n",
      "train loss:2.2918028236231707\n",
      "train loss:2.2868385443092376\n",
      "train loss:2.279114340541758\n",
      "train loss:2.26510043005152\n",
      "train loss:2.2526888742293143\n",
      "train loss:2.222675888745611\n",
      "train loss:2.19425671094475\n",
      "train loss:2.1593311867152694\n",
      "train loss:2.1777261215237314\n",
      "train loss:2.0898340805117375\n",
      "train loss:2.0395785601229464\n",
      "train loss:2.0082072858935045\n",
      "train loss:1.9522222136305034\n",
      "train loss:1.9224940795012069\n",
      "train loss:1.8437196529951194\n",
      "train loss:1.7369911276430559\n",
      "train loss:1.751421095444706\n",
      "train loss:1.59795629232782\n",
      "train loss:1.4823839645326442\n",
      "train loss:1.443677177603905\n",
      "train loss:1.2859825136504002\n",
      "train loss:1.1858811902186785\n",
      "train loss:1.1524210076710935\n",
      "train loss:1.0989740032428081\n",
      "train loss:1.061360715550626\n",
      "train loss:0.9785787008084965\n",
      "train loss:0.8456749912645023\n",
      "train loss:0.9046798396932982\n",
      "train loss:0.7566071674444597\n",
      "train loss:0.8370260687824798\n",
      "train loss:0.6403381377855949\n",
      "train loss:0.6945943460244002\n",
      "train loss:0.7104821593654012\n",
      "train loss:0.8814227429413759\n",
      "train loss:0.669386817434707\n",
      "train loss:0.7071652695662674\n",
      "train loss:0.5982605558951427\n",
      "train loss:0.6497528324621945\n",
      "train loss:0.5347729565040823\n",
      "train loss:0.5368144087329241\n",
      "train loss:0.5456203041805229\n",
      "train loss:0.5361354108828398\n",
      "train loss:0.5188689830122221\n",
      "train loss:0.4620047321733062\n",
      "train loss:0.6518892239792339\n",
      "train loss:0.6063258299653635\n",
      "train loss:0.6701620022675532\n",
      "train loss:0.48347067996075377\n",
      "train loss:0.7979590477785414\n",
      "=== epoch:2, train acc:0.84, test acc:0.797 ===\n",
      "train loss:0.44177462741793083\n",
      "train loss:0.606698093000849\n",
      "train loss:0.48673428987831735\n",
      "train loss:0.4614726924110675\n",
      "train loss:0.3903711396722783\n",
      "train loss:0.5074945239917246\n",
      "train loss:0.5764469080752364\n",
      "train loss:0.3944478941470099\n",
      "train loss:0.468791720636659\n",
      "train loss:0.57404723662151\n",
      "train loss:0.5036903955141679\n",
      "train loss:0.44702439750167033\n",
      "train loss:0.558812964744045\n",
      "train loss:0.4859561372381762\n",
      "train loss:0.4887083765027157\n",
      "train loss:0.3707277001907818\n",
      "train loss:0.32909838921835677\n",
      "train loss:0.3632115032206832\n",
      "train loss:0.5038241263596651\n",
      "train loss:0.4356881650386684\n",
      "train loss:0.39933546133715125\n",
      "train loss:0.4021564792556993\n",
      "train loss:0.43392742787823935\n",
      "train loss:0.3912145126871815\n",
      "train loss:0.5553818902473696\n",
      "train loss:0.41911643449404357\n",
      "train loss:0.30361419466507816\n",
      "train loss:0.6131581129129304\n",
      "train loss:0.6926804708295117\n",
      "train loss:0.5512825795063944\n",
      "train loss:0.4747430345574985\n",
      "train loss:0.39089989831537103\n",
      "train loss:0.4120541995840323\n",
      "train loss:0.3695387078557602\n",
      "train loss:0.4590982227439889\n",
      "train loss:0.49169475722219497\n",
      "train loss:0.37530838425141516\n",
      "train loss:0.5548886092168702\n",
      "train loss:0.40122812521089124\n",
      "train loss:0.3465367942742133\n",
      "train loss:0.3624820401304138\n",
      "train loss:0.5141061304102088\n",
      "train loss:0.47618375458207135\n",
      "train loss:0.4130264145355516\n",
      "train loss:0.3244394915801192\n",
      "train loss:0.37086412932965085\n",
      "train loss:0.3290467858769797\n",
      "train loss:0.40328032580962564\n",
      "train loss:0.33382532591995384\n",
      "train loss:0.32128839202924886\n",
      "=== epoch:3, train acc:0.865, test acc:0.858 ===\n",
      "train loss:0.5099219676161906\n",
      "train loss:0.48243053928370094\n",
      "train loss:0.3986841877954393\n",
      "train loss:0.22905669447477908\n",
      "train loss:0.38414635891354443\n",
      "train loss:0.3330312507166409\n",
      "train loss:0.24726843328549503\n",
      "train loss:0.27631179543204704\n",
      "train loss:0.511091914791633\n",
      "train loss:0.39326273371000703\n",
      "train loss:0.2278027191310147\n",
      "train loss:0.4698817350306617\n",
      "train loss:0.4586503326530246\n",
      "train loss:0.25207497224798336\n",
      "train loss:0.270782831450178\n",
      "train loss:0.5121145423955498\n",
      "train loss:0.19634094915127093\n",
      "train loss:0.3312272511608603\n",
      "train loss:0.44763094298790324\n",
      "train loss:0.3344944503112607\n",
      "train loss:0.5580191526217986\n",
      "train loss:0.41034125994758675\n",
      "train loss:0.35669265966286146\n",
      "train loss:0.32656213593950123\n",
      "train loss:0.5091826402710198\n",
      "train loss:0.39257970391629693\n",
      "train loss:0.2313500535884881\n",
      "train loss:0.3193539820144521\n",
      "train loss:0.20956721861459635\n",
      "train loss:0.3564993508126746\n",
      "train loss:0.27793173638932317\n",
      "train loss:0.2549593780071187\n",
      "train loss:0.35611318455765323\n",
      "train loss:0.3890387743026863\n",
      "train loss:0.13756314965498456\n",
      "train loss:0.34337421026689746\n",
      "train loss:0.32126292613310237\n",
      "train loss:0.18062355835386124\n",
      "train loss:0.3699834825983617\n",
      "train loss:0.27716831087579985\n",
      "train loss:0.2256248346450432\n",
      "train loss:0.19897269394967368\n",
      "train loss:0.25736039664437554\n",
      "train loss:0.5096425286554623\n",
      "train loss:0.2693363562927612\n",
      "train loss:0.27008326173966124\n",
      "train loss:0.19243148669895505\n",
      "train loss:0.301592301511697\n",
      "train loss:0.17440828345965112\n",
      "train loss:0.2004571368740972\n",
      "=== epoch:4, train acc:0.887, test acc:0.866 ===\n",
      "train loss:0.2633780841047342\n",
      "train loss:0.26054746292106734\n",
      "train loss:0.13225607804397116\n",
      "train loss:0.2541491832313682\n",
      "train loss:0.1806316603435382\n",
      "train loss:0.3283377343429335\n",
      "train loss:0.2947463153320997\n",
      "train loss:0.19116510870803136\n",
      "train loss:0.22880197952277886\n",
      "train loss:0.2975535367479677\n",
      "train loss:0.2297386481087618\n",
      "train loss:0.26038146144882\n",
      "train loss:0.18962674801250415\n",
      "train loss:0.4012436323447413\n",
      "train loss:0.27174741764207727\n",
      "train loss:0.19349463675024647\n",
      "train loss:0.2650410538532567\n",
      "train loss:0.2796285074681821\n",
      "train loss:0.25365038784681804\n",
      "train loss:0.24726846160911944\n",
      "train loss:0.1515915452152076\n",
      "train loss:0.23589655546561006\n",
      "train loss:0.30808122400730914\n",
      "train loss:0.15639922621184285\n",
      "train loss:0.24429915045696277\n",
      "train loss:0.20907698418903514\n",
      "train loss:0.23390497672299154\n",
      "train loss:0.2572765760900654\n",
      "train loss:0.2410308794368091\n",
      "train loss:0.2976297012448084\n",
      "train loss:0.3228625873439199\n",
      "train loss:0.14429864000641876\n",
      "train loss:0.27523590903618234\n",
      "train loss:0.14659517088925295\n",
      "train loss:0.285197400532871\n",
      "train loss:0.16042569115392435\n",
      "train loss:0.2520884781010311\n",
      "train loss:0.2174312273939366\n",
      "train loss:0.22687902653148562\n",
      "train loss:0.16745398444156495\n",
      "train loss:0.1491708703048152\n",
      "train loss:0.23298866735915333\n",
      "train loss:0.27270868884817356\n",
      "train loss:0.30570201678377057\n",
      "train loss:0.17707415046374192\n",
      "train loss:0.21113034683165272\n",
      "train loss:0.401124815572629\n",
      "train loss:0.2984703967360508\n",
      "train loss:0.4096733903363652\n",
      "train loss:0.27772789005293225\n",
      "=== epoch:5, train acc:0.897, test acc:0.892 ===\n",
      "train loss:0.26544515845053734\n",
      "train loss:0.22211454555905794\n",
      "train loss:0.313969061685549\n",
      "train loss:0.2511149151544375\n",
      "train loss:0.17203853119508186\n",
      "train loss:0.1400853179780489\n",
      "train loss:0.30777905867298894\n",
      "train loss:0.23260700373674642\n",
      "train loss:0.2525902519664349\n",
      "train loss:0.16453834802974374\n",
      "train loss:0.35317446618968945\n",
      "train loss:0.21791252852144932\n",
      "train loss:0.22033103449137215\n",
      "train loss:0.41531972649859805\n",
      "train loss:0.34611185308855424\n",
      "train loss:0.23546143088953309\n",
      "train loss:0.3948002732915457\n",
      "train loss:0.3070328803212002\n",
      "train loss:0.20966111425971526\n",
      "train loss:0.2874477317594044\n",
      "train loss:0.24903872774134786\n",
      "train loss:0.22638244236368554\n",
      "train loss:0.19223534709515427\n",
      "train loss:0.30589989830334297\n",
      "train loss:0.20557174213408513\n",
      "train loss:0.3256269744438323\n",
      "train loss:0.1942398636584012\n",
      "train loss:0.1783587114075557\n",
      "train loss:0.24048135489732433\n",
      "train loss:0.2238886561260106\n",
      "train loss:0.1523904196092422\n",
      "train loss:0.22613235025869113\n",
      "train loss:0.16786783887562418\n",
      "train loss:0.30319171676849366\n",
      "train loss:0.21435172554279197\n",
      "train loss:0.22849989833672685\n",
      "train loss:0.4116050210423174\n",
      "train loss:0.12833087736771778\n",
      "train loss:0.20344897769609369\n",
      "train loss:0.23755446952734285\n",
      "train loss:0.2591906094105406\n",
      "train loss:0.2542811115839946\n",
      "train loss:0.24282047952581856\n",
      "train loss:0.2695993561434179\n",
      "train loss:0.38166677308310554\n",
      "train loss:0.1507583867096898\n",
      "train loss:0.24173211134216013\n",
      "train loss:0.32779871200672983\n",
      "train loss:0.25849345390124595\n",
      "train loss:0.21208562852752894\n",
      "=== epoch:6, train acc:0.922, test acc:0.902 ===\n",
      "train loss:0.17248592493771717\n",
      "train loss:0.17933923518569703\n",
      "train loss:0.15616626257390254\n",
      "train loss:0.22549988570726062\n",
      "train loss:0.25901895741509184\n",
      "train loss:0.1656124641583286\n",
      "train loss:0.30667549255123727\n",
      "train loss:0.19135853869684172\n",
      "train loss:0.41649939773462585\n",
      "train loss:0.22036832942496956\n",
      "train loss:0.1635968762578175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2155572011924116\n",
      "train loss:0.19383529383248205\n",
      "train loss:0.17018696316343201\n",
      "train loss:0.2571638102033181\n",
      "train loss:0.22362194072610464\n",
      "train loss:0.12274134522854116\n",
      "train loss:0.21253434316035133\n",
      "train loss:0.22735039456187256\n",
      "train loss:0.17741156397008084\n",
      "train loss:0.21699514072602485\n",
      "train loss:0.11703835165802177\n",
      "train loss:0.18672505428184766\n",
      "train loss:0.19800754800110087\n",
      "train loss:0.13947613595279618\n",
      "train loss:0.09836874438149247\n",
      "train loss:0.12721570301713567\n",
      "train loss:0.1523536095809933\n",
      "train loss:0.2498194600162801\n",
      "train loss:0.17023736590580316\n",
      "train loss:0.16529290572075245\n",
      "train loss:0.1962534424375578\n",
      "train loss:0.22118585912160327\n",
      "train loss:0.09183183870480949\n",
      "train loss:0.10872715021979927\n",
      "train loss:0.11748027889223873\n",
      "train loss:0.18406519928812698\n",
      "train loss:0.16389069792132058\n",
      "train loss:0.13429582195018688\n",
      "train loss:0.07285770062896021\n",
      "train loss:0.14540159276648265\n",
      "train loss:0.1974992265127386\n",
      "train loss:0.06165054159233267\n",
      "train loss:0.197468507725262\n",
      "train loss:0.12131044081537795\n",
      "train loss:0.16927541499700638\n",
      "train loss:0.11415975813630416\n",
      "train loss:0.10618425316879834\n",
      "train loss:0.2229419712910283\n",
      "train loss:0.16943903008483288\n",
      "=== epoch:7, train acc:0.935, test acc:0.917 ===\n",
      "train loss:0.3381821960706731\n",
      "train loss:0.26063619663580617\n",
      "train loss:0.24974261059874997\n",
      "train loss:0.2387048281998716\n",
      "train loss:0.2418634992334578\n",
      "train loss:0.1776231704539245\n",
      "train loss:0.14460058060488445\n",
      "train loss:0.2712903002520639\n",
      "train loss:0.3614156744721702\n",
      "train loss:0.08953590068961466\n",
      "train loss:0.2903438018743478\n",
      "train loss:0.12734908280268026\n",
      "train loss:0.07567885159003981\n",
      "train loss:0.2796828311879923\n",
      "train loss:0.11194246504500814\n",
      "train loss:0.11263480502156208\n",
      "train loss:0.1583457315155613\n",
      "train loss:0.20488727766319265\n",
      "train loss:0.12084945727679725\n",
      "train loss:0.21351704329480614\n",
      "train loss:0.13092295014247388\n",
      "train loss:0.32687281332416346\n",
      "train loss:0.26096432244687123\n",
      "train loss:0.12322809204095739\n",
      "train loss:0.16679098496525144\n",
      "train loss:0.12575462842909996\n",
      "train loss:0.11294439088205538\n",
      "train loss:0.19354092011843388\n",
      "train loss:0.12516659840007727\n",
      "train loss:0.13934094125499255\n",
      "train loss:0.11214354368183362\n",
      "train loss:0.1961964110106981\n",
      "train loss:0.16692159010312418\n",
      "train loss:0.13615698770131732\n",
      "train loss:0.2190344030125078\n",
      "train loss:0.2534444559814988\n",
      "train loss:0.09932208531281361\n",
      "train loss:0.1347307293392196\n",
      "train loss:0.20562603809967656\n",
      "train loss:0.12957942860378868\n",
      "train loss:0.08198533588738613\n",
      "train loss:0.33632932427324547\n",
      "train loss:0.11165387370130984\n",
      "train loss:0.1641545071719715\n",
      "train loss:0.22465459718523387\n",
      "train loss:0.12416594159170341\n",
      "train loss:0.26814697573830837\n",
      "train loss:0.27025657257982805\n",
      "train loss:0.11597371642571389\n",
      "train loss:0.1721563940737846\n",
      "=== epoch:8, train acc:0.931, test acc:0.913 ===\n",
      "train loss:0.16173699560495913\n",
      "train loss:0.15866955073496775\n",
      "train loss:0.21051092044932868\n",
      "train loss:0.18253565522579848\n",
      "train loss:0.11153715442686007\n",
      "train loss:0.15400545005195812\n",
      "train loss:0.12716317682385125\n",
      "train loss:0.15963013595156872\n",
      "train loss:0.11730537788332769\n",
      "train loss:0.13234633024594994\n",
      "train loss:0.15430162502312592\n",
      "train loss:0.10316736137977794\n",
      "train loss:0.10116000067550864\n",
      "train loss:0.1641565246372626\n",
      "train loss:0.17839616078964363\n",
      "train loss:0.18644370844963984\n",
      "train loss:0.15403515330575884\n",
      "train loss:0.14651498313971925\n",
      "train loss:0.13126798006269835\n",
      "train loss:0.27877634359538567\n",
      "train loss:0.16449238329724591\n",
      "train loss:0.16192606894055767\n",
      "train loss:0.2880850219831281\n",
      "train loss:0.09514420968378788\n",
      "train loss:0.07990614836323605\n",
      "train loss:0.09096530926428191\n",
      "train loss:0.22690748590382956\n",
      "train loss:0.04941403883342823\n",
      "train loss:0.15319445712574892\n",
      "train loss:0.14384393523050895\n",
      "train loss:0.13516546334550297\n",
      "train loss:0.07877325716218109\n",
      "train loss:0.18665053622580433\n",
      "train loss:0.13902560753948404\n",
      "train loss:0.19195616807751587\n",
      "train loss:0.15703373616054417\n",
      "train loss:0.11675844613477496\n",
      "train loss:0.12692484082483843\n",
      "train loss:0.13628486693842315\n",
      "train loss:0.15490595359724524\n",
      "train loss:0.13215452626597174\n",
      "train loss:0.13689922547095823\n",
      "train loss:0.12769523457992932\n",
      "train loss:0.23825035596852706\n",
      "train loss:0.09471414075021171\n",
      "train loss:0.0931970326902976\n",
      "train loss:0.12646719310343899\n",
      "train loss:0.12357631338697887\n",
      "train loss:0.10827859635341876\n",
      "train loss:0.08910206465024499\n",
      "=== epoch:9, train acc:0.949, test acc:0.932 ===\n",
      "train loss:0.1190470338986351\n",
      "train loss:0.1495051583687229\n",
      "train loss:0.10277586897673144\n",
      "train loss:0.16637883560144157\n",
      "train loss:0.06248622064505586\n",
      "train loss:0.09400296782704666\n",
      "train loss:0.09904855072839265\n",
      "train loss:0.07943373587426937\n",
      "train loss:0.10696564477571929\n",
      "train loss:0.1744837931922552\n",
      "train loss:0.11271804027918389\n",
      "train loss:0.061699573539152\n",
      "train loss:0.05808576174222602\n",
      "train loss:0.07297274252053984\n",
      "train loss:0.17513535240674952\n",
      "train loss:0.06957704649386254\n",
      "train loss:0.056085196516973836\n",
      "train loss:0.21004135032022916\n",
      "train loss:0.16172940813005668\n",
      "train loss:0.11021710142952225\n",
      "train loss:0.07676475108995881\n",
      "train loss:0.097845786720673\n",
      "train loss:0.09944396746590867\n",
      "train loss:0.15736758364768005\n",
      "train loss:0.04198587153805385\n",
      "train loss:0.05910802223100008\n",
      "train loss:0.09206865094795083\n",
      "train loss:0.13981761979474183\n",
      "train loss:0.08090463149771993\n",
      "train loss:0.12632945122008982\n",
      "train loss:0.062063341149839524\n",
      "train loss:0.13816840667958913\n",
      "train loss:0.09545534832085074\n",
      "train loss:0.15362055937771182\n",
      "train loss:0.10137501736261158\n",
      "train loss:0.12620263115277233\n",
      "train loss:0.2042025492003448\n",
      "train loss:0.0960013864340748\n",
      "train loss:0.07416358172584218\n",
      "train loss:0.07707775516728976\n",
      "train loss:0.203250385074012\n",
      "train loss:0.127665250977636\n",
      "train loss:0.20514228517219923\n",
      "train loss:0.12989480774100592\n",
      "train loss:0.3416239999130825\n",
      "train loss:0.12668320673489947\n",
      "train loss:0.14693365614456363\n",
      "train loss:0.040477924366544335\n",
      "train loss:0.09966238576545702\n",
      "train loss:0.11781230437441448\n",
      "=== epoch:10, train acc:0.949, test acc:0.919 ===\n",
      "train loss:0.10205591151024469\n",
      "train loss:0.20588735337558312\n",
      "train loss:0.17414670246571531\n",
      "train loss:0.11750115160450784\n",
      "train loss:0.13332774427947813\n",
      "train loss:0.16264459536225773\n",
      "train loss:0.05983861777986025\n",
      "train loss:0.12929494566293231\n",
      "train loss:0.0865242949589242\n",
      "train loss:0.2200599583297086\n",
      "train loss:0.13492999706462247\n",
      "train loss:0.1747275030388332\n",
      "train loss:0.07816700636770324\n",
      "train loss:0.07659110094977174\n",
      "train loss:0.13327433819110948\n",
      "train loss:0.10530325251821354\n",
      "train loss:0.1148383511186875\n",
      "train loss:0.07332435665915496\n",
      "train loss:0.06747615705700213\n",
      "train loss:0.3188007196070411\n",
      "train loss:0.08810766369254329\n",
      "train loss:0.14458048907112098\n",
      "train loss:0.13435499307444934\n",
      "train loss:0.07311798181845885\n",
      "train loss:0.15798315276115532\n",
      "train loss:0.14980119653871943\n",
      "train loss:0.13352365397112873\n",
      "train loss:0.05180890006023075\n",
      "train loss:0.09328220213747798\n",
      "train loss:0.09359628120755771\n",
      "train loss:0.05919800278686965\n",
      "train loss:0.15048874300420792\n",
      "train loss:0.08352472096447575\n",
      "train loss:0.08852411118774181\n",
      "train loss:0.13792048743829632\n",
      "train loss:0.10088582104737163\n",
      "train loss:0.18610211371411495\n",
      "train loss:0.08759939211224033\n",
      "train loss:0.0701931552871025\n",
      "train loss:0.11730510048670176\n",
      "train loss:0.14753038130559026\n",
      "train loss:0.10364218661332142\n",
      "train loss:0.14551207828982662\n",
      "train loss:0.11700643081674675\n",
      "train loss:0.20188660646300127\n",
      "train loss:0.09208784286441171\n",
      "train loss:0.03465980323993822\n",
      "train loss:0.04520989105666283\n",
      "train loss:0.08080499002010541\n",
      "train loss:0.12586888541277072\n",
      "=== epoch:11, train acc:0.957, test acc:0.936 ===\n",
      "train loss:0.1694526273348988\n",
      "train loss:0.053126941035825354\n",
      "train loss:0.22321915621308147\n",
      "train loss:0.0515344161993587\n",
      "train loss:0.2026060571375163\n",
      "train loss:0.07832139727880977\n",
      "train loss:0.050443812498587697\n",
      "train loss:0.09961401615311781\n",
      "train loss:0.11319386140335418\n",
      "train loss:0.14084986756939052\n",
      "train loss:0.10637576552258438\n",
      "train loss:0.032297456257501266\n",
      "train loss:0.07553886500747095\n",
      "train loss:0.05323594274914392\n",
      "train loss:0.14898562981744218\n",
      "train loss:0.08347782535943349\n",
      "train loss:0.0970980787226839\n",
      "train loss:0.1760310442159588\n",
      "train loss:0.11960454305627924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.20454511097732075\n",
      "train loss:0.09014945332856529\n",
      "train loss:0.046556568002241956\n",
      "train loss:0.19194186095351362\n",
      "train loss:0.0689998952585824\n",
      "train loss:0.13049816710720397\n",
      "train loss:0.08411101148943861\n",
      "train loss:0.08204278905318402\n",
      "train loss:0.0474287907692694\n",
      "train loss:0.12193704132030221\n",
      "train loss:0.06544783620904676\n",
      "train loss:0.06736500564819711\n",
      "train loss:0.04839293614398733\n",
      "train loss:0.060506901632563885\n",
      "train loss:0.1187748774955727\n",
      "train loss:0.050273222507635634\n",
      "train loss:0.07146059301163739\n",
      "train loss:0.07649717030941212\n",
      "train loss:0.09533358500537906\n",
      "train loss:0.15855621519912383\n",
      "train loss:0.12002871587750424\n",
      "train loss:0.040851177802838076\n",
      "train loss:0.08174695711316207\n",
      "train loss:0.1144363431836281\n",
      "train loss:0.056089595769562545\n",
      "train loss:0.12622325647223281\n",
      "train loss:0.0590185913159711\n",
      "train loss:0.06697429221987042\n",
      "train loss:0.033739782608903465\n",
      "train loss:0.07659033902963758\n",
      "train loss:0.0988033891273163\n",
      "=== epoch:12, train acc:0.966, test acc:0.939 ===\n",
      "train loss:0.24368112399263217\n",
      "train loss:0.07519199530075592\n",
      "train loss:0.05945529029205668\n",
      "train loss:0.08137051050061758\n",
      "train loss:0.043103316940857804\n",
      "train loss:0.07049072446058008\n",
      "train loss:0.02477238823959719\n",
      "train loss:0.09640001662854773\n",
      "train loss:0.09560891056812727\n",
      "train loss:0.0449578962335272\n",
      "train loss:0.03896518271047592\n",
      "train loss:0.06342041786954566\n",
      "train loss:0.05724033512666611\n",
      "train loss:0.050709791457534584\n",
      "train loss:0.0870967257766356\n",
      "train loss:0.05693807481514652\n",
      "train loss:0.03666788896505436\n",
      "train loss:0.09613748658269548\n",
      "train loss:0.11547436839096126\n",
      "train loss:0.053091542277951216\n",
      "train loss:0.17424855466684622\n",
      "train loss:0.13344322076558174\n",
      "train loss:0.08636741164288124\n",
      "train loss:0.10185447213010278\n",
      "train loss:0.09001955781325945\n",
      "train loss:0.06342999112599178\n",
      "train loss:0.13434139099171377\n",
      "train loss:0.1183661329622931\n",
      "train loss:0.038245850425755304\n",
      "train loss:0.08554404379132013\n",
      "train loss:0.0906527237424093\n",
      "train loss:0.07660238216148108\n",
      "train loss:0.06911251142129082\n",
      "train loss:0.16872797718823349\n",
      "train loss:0.15114160563412718\n",
      "train loss:0.023900479247977943\n",
      "train loss:0.06958491244701534\n",
      "train loss:0.11177498055307923\n",
      "train loss:0.03150229271770446\n",
      "train loss:0.03634763011592601\n",
      "train loss:0.0857782241106818\n",
      "train loss:0.05465135629807027\n",
      "train loss:0.07726156089058978\n",
      "train loss:0.10898791058943359\n",
      "train loss:0.08173411866755799\n",
      "train loss:0.028962203731861032\n",
      "train loss:0.031151471749223593\n",
      "train loss:0.08342183634427759\n",
      "train loss:0.1704945984283005\n",
      "train loss:0.04292692701111701\n",
      "=== epoch:13, train acc:0.969, test acc:0.946 ===\n",
      "train loss:0.0732935224262478\n",
      "train loss:0.1178002728677712\n",
      "train loss:0.03362717430796224\n",
      "train loss:0.10582796876245563\n",
      "train loss:0.0922980976771515\n",
      "train loss:0.044519679536910585\n",
      "train loss:0.05567097437614839\n",
      "train loss:0.044306577436682065\n",
      "train loss:0.08046705043989937\n",
      "train loss:0.06537825261810797\n",
      "train loss:0.08223990627517704\n",
      "train loss:0.04291107195327734\n",
      "train loss:0.056209288057608574\n",
      "train loss:0.06823568631733867\n",
      "train loss:0.05941404749843504\n",
      "train loss:0.06528706637332837\n",
      "train loss:0.06664857904667357\n",
      "train loss:0.0897931021601109\n",
      "train loss:0.08742514698061334\n",
      "train loss:0.09583687438037712\n",
      "train loss:0.15030509659718713\n",
      "train loss:0.07614133496995365\n",
      "train loss:0.12509540169887562\n",
      "train loss:0.06014516489526666\n",
      "train loss:0.05128916262276656\n",
      "train loss:0.03798848021530646\n",
      "train loss:0.10236196765093458\n",
      "train loss:0.06540400120158774\n",
      "train loss:0.021326533844651913\n",
      "train loss:0.11205835667140271\n",
      "train loss:0.05796704779138841\n",
      "train loss:0.10490082655376204\n",
      "train loss:0.03854253780786542\n",
      "train loss:0.0935778181223725\n",
      "train loss:0.0441433385795654\n",
      "train loss:0.07799911498131139\n",
      "train loss:0.03570504334715866\n",
      "train loss:0.029380363693862033\n",
      "train loss:0.12675757805663293\n",
      "train loss:0.046739407826435014\n",
      "train loss:0.049879438571599995\n",
      "train loss:0.058819489038731104\n",
      "train loss:0.031136819408275156\n",
      "train loss:0.031565789509640105\n",
      "train loss:0.12035149470671314\n",
      "train loss:0.06828268605878612\n",
      "train loss:0.11374885075468467\n",
      "train loss:0.035610986876234686\n",
      "train loss:0.11303832821036754\n",
      "train loss:0.1506278711170906\n",
      "=== epoch:14, train acc:0.977, test acc:0.952 ===\n",
      "train loss:0.04959675926378718\n",
      "train loss:0.058689472980334285\n",
      "train loss:0.04943169479774842\n",
      "train loss:0.07984157810469596\n",
      "train loss:0.026760804520712823\n",
      "train loss:0.02534021653230523\n",
      "train loss:0.06856288404862206\n",
      "train loss:0.08923213793235198\n",
      "train loss:0.05986950150846218\n",
      "train loss:0.07356816944839084\n",
      "train loss:0.03140042915148942\n",
      "train loss:0.05525622388122724\n",
      "train loss:0.030033231537880987\n",
      "train loss:0.05641461444836817\n",
      "train loss:0.1560975960833962\n",
      "train loss:0.05465838739741225\n",
      "train loss:0.0492425048250526\n",
      "train loss:0.035544082886218825\n",
      "train loss:0.05249694376926639\n",
      "train loss:0.05078088273889807\n",
      "train loss:0.04984664794265447\n",
      "train loss:0.030642703822935418\n",
      "train loss:0.03457748247375055\n",
      "train loss:0.03217244607266075\n",
      "train loss:0.035942971365992366\n",
      "train loss:0.09039105809301069\n",
      "train loss:0.021701780155713234\n",
      "train loss:0.04936307565350535\n",
      "train loss:0.09973620985348916\n",
      "train loss:0.084781447423175\n",
      "train loss:0.06898194803813411\n",
      "train loss:0.035866221848891267\n",
      "train loss:0.09765006080967825\n",
      "train loss:0.04345471671274167\n",
      "train loss:0.06620059543635007\n",
      "train loss:0.05180079850478699\n",
      "train loss:0.055284002126677834\n",
      "train loss:0.06545416275602475\n",
      "train loss:0.03727623484242312\n",
      "train loss:0.06586525737598761\n",
      "train loss:0.08642793448168944\n",
      "train loss:0.05478105154936469\n",
      "train loss:0.05654410429017573\n",
      "train loss:0.04130618819794631\n",
      "train loss:0.04766181134474391\n",
      "train loss:0.11397207690581203\n",
      "train loss:0.08812232466230116\n",
      "train loss:0.0628442582968339\n",
      "train loss:0.09263340724252084\n",
      "train loss:0.12115407590279276\n",
      "=== epoch:15, train acc:0.981, test acc:0.951 ===\n",
      "train loss:0.033811233266524174\n",
      "train loss:0.05363071506170751\n",
      "train loss:0.03122209384407605\n",
      "train loss:0.030918876531109564\n",
      "train loss:0.0869539699349931\n",
      "train loss:0.1144796572586403\n",
      "train loss:0.07305431680592311\n",
      "train loss:0.06521948424143499\n",
      "train loss:0.07512649967841503\n",
      "train loss:0.0394516287486374\n",
      "train loss:0.060199856351426974\n",
      "train loss:0.1148901443314561\n",
      "train loss:0.047752648826681475\n",
      "train loss:0.034666323693765995\n",
      "train loss:0.04231045332596413\n",
      "train loss:0.048326432199247996\n",
      "train loss:0.034149640502919916\n",
      "train loss:0.06428758736713706\n",
      "train loss:0.05095278020566979\n",
      "train loss:0.09827730641094082\n",
      "train loss:0.11615718163243713\n",
      "train loss:0.06532858437616165\n",
      "train loss:0.10467852745327942\n",
      "train loss:0.10454502083241182\n",
      "train loss:0.08441358404173124\n",
      "train loss:0.02991319034867856\n",
      "train loss:0.06438656517892256\n",
      "train loss:0.024012099540007513\n",
      "train loss:0.029781479169454773\n",
      "train loss:0.07002297130478152\n",
      "train loss:0.03195879044390552\n",
      "train loss:0.04535869971313185\n",
      "train loss:0.03270969005542338\n",
      "train loss:0.07920040461463183\n",
      "train loss:0.056539953696085554\n",
      "train loss:0.03245269141514313\n",
      "train loss:0.053408473165185086\n",
      "train loss:0.061956501674174866\n",
      "train loss:0.06388693446228305\n",
      "train loss:0.06808838750320403\n",
      "train loss:0.10124491112996942\n",
      "train loss:0.014710073215906337\n",
      "train loss:0.06825442008133958\n",
      "train loss:0.06336282878851444\n",
      "train loss:0.020440207287520516\n",
      "train loss:0.0209574010955901\n",
      "train loss:0.042235052265741146\n",
      "train loss:0.056016639658281714\n",
      "train loss:0.05340917367427682\n",
      "train loss:0.05003932292916735\n",
      "=== epoch:16, train acc:0.984, test acc:0.955 ===\n",
      "train loss:0.036028101547850695\n",
      "train loss:0.08765774364496455\n",
      "train loss:0.03953358056774783\n",
      "train loss:0.05109117046760501\n",
      "train loss:0.010030797103402753\n",
      "train loss:0.02765186490001113\n",
      "train loss:0.06158124120634154\n",
      "train loss:0.08784092901307845\n",
      "train loss:0.02401755076157261\n",
      "train loss:0.0336066940402376\n",
      "train loss:0.06717438153157998\n",
      "train loss:0.08617152391398186\n",
      "train loss:0.03737105970303164\n",
      "train loss:0.028144978112138928\n",
      "train loss:0.03170894500666475\n",
      "train loss:0.04258013959157104\n",
      "train loss:0.025949143012915035\n",
      "train loss:0.014325505486629461\n",
      "train loss:0.02743875102970264\n",
      "train loss:0.08199401104267823\n",
      "train loss:0.020150838837447345\n",
      "train loss:0.03250219123649865\n",
      "train loss:0.04671790834395858\n",
      "train loss:0.14095835928861158\n",
      "train loss:0.0748202889291158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.022613708673804987\n",
      "train loss:0.0944189489292665\n",
      "train loss:0.022814673910742166\n",
      "train loss:0.030138370540268387\n",
      "train loss:0.03979114686436291\n",
      "train loss:0.03348312551537811\n",
      "train loss:0.038785254049523876\n",
      "train loss:0.055303321618583604\n",
      "train loss:0.04200204418739508\n",
      "train loss:0.02003010516871687\n",
      "train loss:0.01841891676738719\n",
      "train loss:0.02091864725163963\n",
      "train loss:0.043760639600481224\n",
      "train loss:0.03901316768796463\n",
      "train loss:0.036151953359472154\n",
      "train loss:0.03169526417517278\n",
      "train loss:0.04238218395062942\n",
      "train loss:0.02003122950544273\n",
      "train loss:0.02628229426645425\n",
      "train loss:0.02053940313019337\n",
      "train loss:0.04176609600632214\n",
      "train loss:0.04853896088061696\n",
      "train loss:0.035316809727658736\n",
      "train loss:0.02436579340196324\n",
      "train loss:0.06765951357145847\n",
      "=== epoch:17, train acc:0.986, test acc:0.961 ===\n",
      "train loss:0.012984152035661587\n",
      "train loss:0.0651235067642105\n",
      "train loss:0.07542371431974061\n",
      "train loss:0.01931198783663897\n",
      "train loss:0.060400405375151796\n",
      "train loss:0.031510842739780706\n",
      "train loss:0.04226363101856233\n",
      "train loss:0.0256874166210938\n",
      "train loss:0.058897396238029275\n",
      "train loss:0.05325011611396466\n",
      "train loss:0.01968355039234176\n",
      "train loss:0.025540156421424237\n",
      "train loss:0.04662738631441079\n",
      "train loss:0.03828256159635614\n",
      "train loss:0.02058728599577103\n",
      "train loss:0.03300887422675898\n",
      "train loss:0.023977942969057017\n",
      "train loss:0.052249232713793586\n",
      "train loss:0.03367039776736497\n",
      "train loss:0.01579221167184741\n",
      "train loss:0.06526476122611666\n",
      "train loss:0.026313993013961506\n",
      "train loss:0.03845910329870404\n",
      "train loss:0.05603203880949388\n",
      "train loss:0.0230567093873875\n",
      "train loss:0.03902886929051439\n",
      "train loss:0.03618781688018367\n",
      "train loss:0.053401557038086356\n",
      "train loss:0.017623963037181157\n",
      "train loss:0.03298827956390252\n",
      "train loss:0.046909353180249955\n",
      "train loss:0.03614298002253146\n",
      "train loss:0.021002505148606146\n",
      "train loss:0.018988566002640258\n",
      "train loss:0.020589567752631818\n",
      "train loss:0.030041041232054436\n",
      "train loss:0.048766051720123944\n",
      "train loss:0.08662599259832823\n",
      "train loss:0.06433112297911411\n",
      "train loss:0.039573354177775216\n",
      "train loss:0.05146703795352178\n",
      "train loss:0.03323352567049299\n",
      "train loss:0.06574680050809358\n",
      "train loss:0.11476481783492946\n",
      "train loss:0.017329095653420497\n",
      "train loss:0.013936353748939556\n",
      "train loss:0.01559560356979083\n",
      "train loss:0.012850005115495585\n",
      "train loss:0.02512554080694528\n",
      "train loss:0.01454749533682022\n",
      "=== epoch:18, train acc:0.986, test acc:0.964 ===\n",
      "train loss:0.015564407424449353\n",
      "train loss:0.031056433093696878\n",
      "train loss:0.10444649241688626\n",
      "train loss:0.036790568704579335\n",
      "train loss:0.045578749997173686\n",
      "train loss:0.017169297188510957\n",
      "train loss:0.042938208743583735\n",
      "train loss:0.014400140177474126\n",
      "train loss:0.059316954622448265\n",
      "train loss:0.015204711951579939\n",
      "train loss:0.03408145032216271\n",
      "train loss:0.006509562998914498\n",
      "train loss:0.05892974527910332\n",
      "train loss:0.02434565730551554\n",
      "train loss:0.029269324802227294\n",
      "train loss:0.012241459282024261\n",
      "train loss:0.03516875868117463\n",
      "train loss:0.02054403087766663\n",
      "train loss:0.03338796574323469\n",
      "train loss:0.06400727665771626\n",
      "train loss:0.03720727449096582\n",
      "train loss:0.018932500859893137\n",
      "train loss:0.06608329997707979\n",
      "train loss:0.03772805533562294\n",
      "train loss:0.036555562890003446\n",
      "train loss:0.0184504675174839\n",
      "train loss:0.05001832037896947\n",
      "train loss:0.031937419539869144\n",
      "train loss:0.016681462882754444\n",
      "train loss:0.04357893773271445\n",
      "train loss:0.044163225148703314\n",
      "train loss:0.025908298318815467\n",
      "train loss:0.016770911576097693\n",
      "train loss:0.034047066011908315\n",
      "train loss:0.018227386926914696\n",
      "train loss:0.07290634277101458\n",
      "train loss:0.01876188830169734\n",
      "train loss:0.050411775142376596\n",
      "train loss:0.047140377898114566\n",
      "train loss:0.045098256531147234\n",
      "train loss:0.02232428872951913\n",
      "train loss:0.014416591012764155\n",
      "train loss:0.01131611632363393\n",
      "train loss:0.017274676831700284\n",
      "train loss:0.011797077046221138\n",
      "train loss:0.0406968261497485\n",
      "train loss:0.020646566505553503\n",
      "train loss:0.021451170750709254\n",
      "train loss:0.04924207329792116\n",
      "train loss:0.016548624437569714\n",
      "=== epoch:19, train acc:0.992, test acc:0.963 ===\n",
      "train loss:0.017409903115323717\n",
      "train loss:0.04456599183080642\n",
      "train loss:0.03492867270393638\n",
      "train loss:0.04430836471617972\n",
      "train loss:0.0441680903116594\n",
      "train loss:0.02377734107473077\n",
      "train loss:0.07379043536821665\n",
      "train loss:0.009793895140711684\n",
      "train loss:0.019456034308463398\n",
      "train loss:0.019018030934254374\n",
      "train loss:0.029341337225973863\n",
      "train loss:0.04601299734455008\n",
      "train loss:0.013490156798893198\n",
      "train loss:0.02725630204374724\n",
      "train loss:0.030639265936167746\n",
      "train loss:0.0137444833533813\n",
      "train loss:0.022034509601476984\n",
      "train loss:0.038625286372127074\n",
      "train loss:0.07635707479510143\n",
      "train loss:0.00663546546167893\n",
      "train loss:0.026090088922925286\n",
      "train loss:0.014319980959415722\n",
      "train loss:0.03915086282134932\n",
      "train loss:0.042268976312644\n",
      "train loss:0.03169033021081422\n",
      "train loss:0.01566954053166354\n",
      "train loss:0.006819936116977649\n",
      "train loss:0.017395771369092105\n",
      "train loss:0.02901510042679244\n",
      "train loss:0.04141613674755452\n",
      "train loss:0.013788477591189981\n",
      "train loss:0.009448191748406944\n",
      "train loss:0.01832903030040418\n",
      "train loss:0.029336585910916625\n",
      "train loss:0.025873991969142543\n",
      "train loss:0.05167006762694009\n",
      "train loss:0.013257702935220819\n",
      "train loss:0.060291544304836096\n",
      "train loss:0.006365239698401716\n",
      "train loss:0.014496431452001312\n",
      "train loss:0.03547424858476793\n",
      "train loss:0.009848903945128556\n",
      "train loss:0.015065921893110464\n",
      "train loss:0.010864240999747256\n",
      "train loss:0.026232994565279822\n",
      "train loss:0.01739022759371304\n",
      "train loss:0.01740308273015687\n",
      "train loss:0.011396186634819463\n",
      "train loss:0.0306868617619776\n",
      "train loss:0.01954295683870039\n",
      "=== epoch:20, train acc:0.992, test acc:0.962 ===\n",
      "train loss:0.022238741138569364\n",
      "train loss:0.020744741959064485\n",
      "train loss:0.02677249913814122\n",
      "train loss:0.012251228947521751\n",
      "train loss:0.017339218183642136\n",
      "train loss:0.033844426790935175\n",
      "train loss:0.02041137328681625\n",
      "train loss:0.009496583772704782\n",
      "train loss:0.02121024278203841\n",
      "train loss:0.0196668062756944\n",
      "train loss:0.0173226378668813\n",
      "train loss:0.056088621269067725\n",
      "train loss:0.015909710290424633\n",
      "train loss:0.04218627337107658\n",
      "train loss:0.0074411566221992435\n",
      "train loss:0.011218147571399512\n",
      "train loss:0.026050260371490608\n",
      "train loss:0.02315291688632326\n",
      "train loss:0.02885979992188261\n",
      "train loss:0.02654988251915795\n",
      "train loss:0.03123974890754366\n",
      "train loss:0.027943409903559945\n",
      "train loss:0.01520567168843094\n",
      "train loss:0.016890667739811325\n",
      "train loss:0.02956244577706652\n",
      "train loss:0.015247371304705293\n",
      "train loss:0.02287328946964854\n",
      "train loss:0.006568267049391456\n",
      "train loss:0.0115320654283928\n",
      "train loss:0.014761757191560974\n",
      "train loss:0.0614894079326698\n",
      "train loss:0.03197995112380619\n",
      "train loss:0.01171392572872209\n",
      "train loss:0.03238672878023377\n",
      "train loss:0.014691980559667356\n",
      "train loss:0.021392210374017978\n",
      "train loss:0.02593655812524168\n",
      "train loss:0.03474023825109412\n",
      "train loss:0.014060763330870857\n",
      "train loss:0.028558259947877373\n",
      "train loss:0.012451342554407083\n",
      "train loss:0.01625551568280376\n",
      "train loss:0.016684832220494057\n",
      "train loss:0.012882878516480121\n",
      "train loss:0.025152312846686998\n",
      "train loss:0.03728857357147319\n",
      "train loss:0.006834006964143242\n",
      "train loss:0.015103259880819444\n",
      "train loss:0.033631536218013146\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.956\n",
      "Save Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(\n",
    "    input_dim=(1, 28, 28),\n",
    "    conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "    hidden_size=100,\n",
    "    output_size=10,\n",
    "    weight_init_std=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    network, x_train, t_train, x_test, t_test, epochs=max_epochs, mini_batch_size=100, optimizer='Adam',\n",
    "    optimizer_param={'lr': 0.001}, evaluate_sample_num_per_epoch=1000\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "network.save_params('params.pkl')\n",
    "print('Save Network Parameters!')\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36694.244163,
   "end_time": "2022-06-01T15:52:45.852922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-01T05:41:11.608759",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
