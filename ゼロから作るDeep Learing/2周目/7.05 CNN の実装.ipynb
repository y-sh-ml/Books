{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beca3f23",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005741,
     "end_time": "2022-06-01T05:41:22.715773",
     "exception": false,
     "start_time": "2022-06-01T05:41:22.710032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7.5　CNN の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595f23ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T05:41:22.730774Z",
     "iopub.status.busy": "2022-06-01T05:41:22.729790Z",
     "iopub.status.idle": "2022-06-01T05:41:22.745433Z",
     "shell.execute_reply": "2022-06-01T05:41:22.744186Z"
    },
    "papermill": {
     "duration": 0.024976,
     "end_time": "2022-06-01T05:41:22.748380",
     "exception": false,
     "start_time": "2022-06-01T05:41:22.723404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26a56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/yuta.shimizu/Downloads/Machine Learning/deep-learning-from-scratch-master/ch07')\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e7547",
   "metadata": {},
   "source": [
    "### 7.5.1　CNN の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7143ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(\n",
    "        self, input_dim=(1, 28, 28),\n",
    "        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "        hidden_size=100,\n",
    "        out_size=10,\n",
    "        weight_init_std=0.01\n",
    "    ):\n",
    "        \n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        def predict(self, x):\n",
    "            for layer in self.layers.values():\n",
    "                x = layer.forward(x)\n",
    "            return x\n",
    "        \n",
    "        def loss(self, x, t):\n",
    "            y = self.predict(x)\n",
    "            return self.last_layer.forward(y, t)\n",
    "        \n",
    "        def gradient(self, x, t):\n",
    "            self.loss(x, t)\n",
    "            \n",
    "            dout = 1\n",
    "            dout = self.last_layer.backward(dout)\n",
    "            \n",
    "            layers = list(self.layers.values())\n",
    "            layers.reverse()\n",
    "            for layer in layers:\n",
    "                dout = layer.backward(dout)\n",
    "                \n",
    "            grads = {}\n",
    "            grads['W1'] = self.layers['Conv1'].dW\n",
    "            grads['b1'] = self.layers['Conv1'].db\n",
    "            grads['W2'] = self.layers['Affine1'].dW\n",
    "            grads['b2'] = self.layers['Affine1'].db\n",
    "            grads['W3'] = self.layers['Affine2'].dW\n",
    "            grads['b3'] = self.layers['Affine2'].db\n",
    "            \n",
    "            return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4ba38",
   "metadata": {},
   "source": [
    "### 7.5.2　CNN の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a4b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160b9a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2995018992337055\n",
      "=== epoch:1, train acc:0.23, test acc:0.179 ===\n",
      "train loss:2.295291231782306\n",
      "train loss:2.290616283538021\n",
      "train loss:2.2836367835897273\n",
      "train loss:2.2774860124217553\n",
      "train loss:2.264658156765721\n",
      "train loss:2.2420232319033455\n",
      "train loss:2.2257162229046448\n",
      "train loss:2.2107930512101945\n",
      "train loss:2.1366527912175304\n",
      "train loss:2.127408486694892\n",
      "train loss:2.057087708758809\n",
      "train loss:2.0688045600177465\n",
      "train loss:2.0297366099253678\n",
      "train loss:1.9708904038570654\n",
      "train loss:1.873924361123789\n",
      "train loss:1.7871700875302545\n",
      "train loss:1.7684409397428533\n",
      "train loss:1.6543867554232932\n",
      "train loss:1.5392893723112724\n",
      "train loss:1.5280364049789028\n",
      "train loss:1.5090387370078693\n",
      "train loss:1.415972733294693\n",
      "train loss:1.2511161322152728\n",
      "train loss:1.3988765641957395\n",
      "train loss:1.156379041993848\n",
      "train loss:1.117887797727329\n",
      "train loss:1.049396080047496\n",
      "train loss:0.9752794124731468\n",
      "train loss:0.8739412040329292\n",
      "train loss:0.8947948376998238\n",
      "train loss:1.0319603576110543\n",
      "train loss:0.6422040871778243\n",
      "train loss:0.7649791196596909\n",
      "train loss:0.7573569401294304\n",
      "train loss:0.7925147985482637\n",
      "train loss:0.7701808604082365\n",
      "train loss:0.6824558301166289\n",
      "train loss:0.7156313675014856\n",
      "train loss:0.7781085923900163\n",
      "train loss:0.7956716196365224\n",
      "train loss:0.5610313852390305\n",
      "train loss:0.6229062561797114\n",
      "train loss:0.6847055977669352\n",
      "train loss:0.6231621381289509\n",
      "train loss:0.5599229117680748\n",
      "train loss:0.6714319716006598\n",
      "train loss:0.5119094330927464\n",
      "train loss:0.6937511156070311\n",
      "train loss:0.7124886540155052\n",
      "train loss:0.5172197667061692\n",
      "=== epoch:2, train acc:0.802, test acc:0.785 ===\n",
      "train loss:0.47590974527612473\n",
      "train loss:0.42490865758545804\n",
      "train loss:0.5448995818765863\n",
      "train loss:0.516884438774631\n",
      "train loss:0.4006669993044605\n",
      "train loss:0.45630414712279915\n",
      "train loss:0.4089230244944475\n",
      "train loss:0.4092338879925347\n",
      "train loss:0.3750909517824242\n",
      "train loss:0.47258170201816624\n",
      "train loss:0.3853963114333962\n",
      "train loss:0.4597605076948512\n",
      "train loss:0.582508642192402\n",
      "train loss:0.5094730805409003\n",
      "train loss:0.5681920448678057\n",
      "train loss:0.3322216117555047\n",
      "train loss:0.31797393641563365\n",
      "train loss:0.30480162750217443\n",
      "train loss:0.4482492537027584\n",
      "train loss:0.46123138451901846\n",
      "train loss:0.5017956645098574\n",
      "train loss:0.560782081351374\n",
      "train loss:0.34462301470354595\n",
      "train loss:0.33190903474374694\n",
      "train loss:0.3810428481415166\n",
      "train loss:0.5106554949026897\n",
      "train loss:0.4916025345167874\n",
      "train loss:0.29073921073076514\n",
      "train loss:0.39934482875269883\n",
      "train loss:0.539824460486531\n",
      "train loss:0.3924556158654998\n",
      "train loss:0.3506245528093171\n",
      "train loss:0.3808826340613613\n",
      "train loss:0.4697283485934702\n",
      "train loss:0.38804089704149436\n",
      "train loss:0.3351103789425609\n",
      "train loss:0.45459350442021956\n",
      "train loss:0.40114784456467467\n",
      "train loss:0.3934675730863134\n",
      "train loss:0.3412691860350634\n",
      "train loss:0.5836145353440084\n",
      "train loss:0.2381651653458757\n",
      "train loss:0.47837103557986027\n",
      "train loss:0.39073046954410023\n",
      "train loss:0.3797658651891825\n",
      "train loss:0.3308341309768013\n",
      "train loss:0.4491782590929483\n",
      "train loss:0.37057027539054566\n",
      "train loss:0.3611776344087888\n",
      "train loss:0.48057251571645343\n",
      "=== epoch:3, train acc:0.869, test acc:0.863 ===\n",
      "train loss:0.3217790141712183\n",
      "train loss:0.47274022820268613\n",
      "train loss:0.3985115120552789\n",
      "train loss:0.265130541441895\n",
      "train loss:0.40254934323333835\n",
      "train loss:0.19435455775156424\n",
      "train loss:0.23493453583197643\n",
      "train loss:0.2176964211126132\n",
      "train loss:0.38598733843690036\n",
      "train loss:0.5552815733357352\n",
      "train loss:0.24926126321740505\n",
      "train loss:0.24360797305375922\n",
      "train loss:0.2579358586172595\n",
      "train loss:0.35193706105517997\n",
      "train loss:0.3238871338322038\n",
      "train loss:0.30240348141358286\n",
      "train loss:0.20474339517555848\n",
      "train loss:0.3007623467236614\n",
      "train loss:0.4852453284037005\n",
      "train loss:0.23927741146795178\n",
      "train loss:0.25732245028505907\n",
      "train loss:0.199742133161321\n",
      "train loss:0.46874079543032965\n",
      "train loss:0.21970198170439414\n",
      "train loss:0.49172160518010544\n",
      "train loss:0.3365287525865764\n",
      "train loss:0.2794318645028211\n",
      "train loss:0.303170077383045\n",
      "train loss:0.3543550482228251\n",
      "train loss:0.33641470717753136\n",
      "train loss:0.3081577457429006\n",
      "train loss:0.4771540784757899\n",
      "train loss:0.27855526554623733\n",
      "train loss:0.22862990031434446\n",
      "train loss:0.3628696687179762\n",
      "train loss:0.28578812966681466\n",
      "train loss:0.21784313040592831\n",
      "train loss:0.27577668340179284\n",
      "train loss:0.37392146004937243\n",
      "train loss:0.15723330144775471\n",
      "train loss:0.23911706250671474\n",
      "train loss:0.28281953819357797\n",
      "train loss:0.2369072287042093\n",
      "train loss:0.326120772910803\n",
      "train loss:0.219261223962468\n",
      "train loss:0.36794261783593074\n",
      "train loss:0.15152076667733516\n",
      "train loss:0.26583880817340577\n",
      "train loss:0.2777047904433249\n",
      "train loss:0.24783170510873082\n",
      "=== epoch:4, train acc:0.904, test acc:0.875 ===\n",
      "train loss:0.2039910972646395\n",
      "train loss:0.256235336976183\n",
      "train loss:0.5087579828148907\n",
      "train loss:0.19026044659662392\n",
      "train loss:0.23926234098641289\n",
      "train loss:0.29766197254856314\n",
      "train loss:0.3339254574253942\n",
      "train loss:0.2960071374117994\n",
      "train loss:0.2539840573639617\n",
      "train loss:0.24496739156553698\n",
      "train loss:0.3447568060022394\n",
      "train loss:0.33764524312568256\n",
      "train loss:0.25743600012056705\n",
      "train loss:0.2167210184857625\n",
      "train loss:0.22803166052247723\n",
      "train loss:0.3086173312291458\n",
      "train loss:0.3408132431533176\n",
      "train loss:0.27668085812837107\n",
      "train loss:0.21645224092211612\n",
      "train loss:0.38099754573942307\n",
      "train loss:0.18488851194247913\n",
      "train loss:0.23202169184450264\n",
      "train loss:0.2562320054171202\n",
      "train loss:0.14919774306809439\n",
      "train loss:0.22320682877394846\n",
      "train loss:0.1960531134589918\n",
      "train loss:0.5115747166514502\n",
      "train loss:0.3249631580824283\n",
      "train loss:0.17738986669724016\n",
      "train loss:0.28331611127055345\n",
      "train loss:0.16337262385365378\n",
      "train loss:0.24352433214014088\n",
      "train loss:0.2209554457355672\n",
      "train loss:0.23872431317833928\n",
      "train loss:0.24761582643819957\n",
      "train loss:0.30258951583323845\n",
      "train loss:0.3668094123694011\n",
      "train loss:0.3961257093272908\n",
      "train loss:0.2726636075153522\n",
      "train loss:0.23358115002728128\n",
      "train loss:0.27054357814699953\n",
      "train loss:0.29820529320848943\n",
      "train loss:0.31702939807144453\n",
      "train loss:0.21490133689380492\n",
      "train loss:0.2149015799474851\n",
      "train loss:0.17015618134709737\n",
      "train loss:0.24727096327081466\n",
      "train loss:0.13830664689005887\n",
      "train loss:0.27446304686031153\n",
      "train loss:0.18232248041105165\n",
      "=== epoch:5, train acc:0.909, test acc:0.898 ===\n",
      "train loss:0.25226369737951054\n",
      "train loss:0.30074200655565436\n",
      "train loss:0.15785204052461066\n",
      "train loss:0.22502452399443348\n",
      "train loss:0.1692881629502901\n",
      "train loss:0.18511748817190804\n",
      "train loss:0.19229207086108746\n",
      "train loss:0.24627906132587848\n",
      "train loss:0.1752326657679538\n",
      "train loss:0.22599852304685622\n",
      "train loss:0.11117487515769531\n",
      "train loss:0.19563195668464664\n",
      "train loss:0.24391832812633743\n",
      "train loss:0.2630365034342422\n",
      "train loss:0.1830793919149521\n",
      "train loss:0.3132906804255579\n",
      "train loss:0.14527982674305595\n",
      "train loss:0.2428670115136435\n",
      "train loss:0.15528045478646124\n",
      "train loss:0.20077362198113122\n",
      "train loss:0.1949354923205032\n",
      "train loss:0.12467469428211178\n",
      "train loss:0.2261600596336352\n",
      "train loss:0.3243505067288806\n",
      "train loss:0.2852659041225718\n",
      "train loss:0.15561643325450733\n",
      "train loss:0.2202437406942604\n",
      "train loss:0.19755098217480974\n",
      "train loss:0.23068907531710728\n",
      "train loss:0.21657174699905862\n",
      "train loss:0.14044398321201673\n",
      "train loss:0.35657137038116843\n",
      "train loss:0.14175171272643822\n",
      "train loss:0.32279026361828606\n",
      "train loss:0.2026162008890248\n",
      "train loss:0.2788733223610791\n",
      "train loss:0.1210988104619369\n",
      "train loss:0.13475812357852665\n",
      "train loss:0.11802951472708056\n",
      "train loss:0.14771048518902657\n",
      "train loss:0.17316565533565562\n",
      "train loss:0.2002678769850515\n",
      "train loss:0.2501387944784868\n",
      "train loss:0.12065704910884993\n",
      "train loss:0.2667600418894013\n",
      "train loss:0.1855765109931402\n",
      "train loss:0.24279208474009095\n",
      "train loss:0.2940040278830798\n",
      "train loss:0.11668497230714364\n",
      "train loss:0.2829565894530309\n",
      "=== epoch:6, train acc:0.922, test acc:0.89 ===\n",
      "train loss:0.2889228297266336\n",
      "train loss:0.15884443893005776\n",
      "train loss:0.14119774660348225\n",
      "train loss:0.19146061883858992\n",
      "train loss:0.12586671967597607\n",
      "train loss:0.31580718393807855\n",
      "train loss:0.07630958197922293\n",
      "train loss:0.22892305927534987\n",
      "train loss:0.2627417535661434\n",
      "train loss:0.22467737961501857\n",
      "train loss:0.10136425123624149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.12200291337373505\n",
      "train loss:0.2756880038438071\n",
      "train loss:0.11659924146657467\n",
      "train loss:0.25050717632090885\n",
      "train loss:0.14459118451173056\n",
      "train loss:0.2349684345346847\n",
      "train loss:0.194939613392458\n",
      "train loss:0.4437852086808144\n",
      "train loss:0.2250298238307763\n",
      "train loss:0.17069575243621832\n",
      "train loss:0.1449846501526704\n",
      "train loss:0.16361605785624372\n",
      "train loss:0.12661158415848442\n",
      "train loss:0.13037123651511562\n",
      "train loss:0.23800802923511566\n",
      "train loss:0.22895836648515652\n",
      "train loss:0.18101739002431352\n",
      "train loss:0.17917666719100836\n",
      "train loss:0.2335713786180148\n",
      "train loss:0.18207578412822323\n",
      "train loss:0.12990355810023632\n",
      "train loss:0.11638945447035569\n",
      "train loss:0.14122092581057033\n",
      "train loss:0.24089723071831098\n",
      "train loss:0.1479254585847375\n",
      "train loss:0.2590240322426583\n",
      "train loss:0.10524567185828658\n",
      "train loss:0.24449396083436195\n",
      "train loss:0.17309236908178127\n",
      "train loss:0.13816146243544403\n",
      "train loss:0.1182706554938534\n",
      "train loss:0.1944872548591324\n",
      "train loss:0.11236980009488558\n",
      "train loss:0.23556856408166535\n",
      "train loss:0.15779096979305368\n",
      "train loss:0.23639112251521854\n",
      "train loss:0.1275310002456392\n",
      "train loss:0.09626254704486692\n",
      "train loss:0.08476144330719712\n",
      "=== epoch:7, train acc:0.931, test acc:0.913 ===\n",
      "train loss:0.0796864704229389\n",
      "train loss:0.12865884396812277\n",
      "train loss:0.19786933825288197\n",
      "train loss:0.10698986505847963\n",
      "train loss:0.07669777554067973\n",
      "train loss:0.13904388620601305\n",
      "train loss:0.17119688589715218\n",
      "train loss:0.15410874846224806\n",
      "train loss:0.1276879979862686\n",
      "train loss:0.18312644476309198\n",
      "train loss:0.2590093140776761\n",
      "train loss:0.20972622910384822\n",
      "train loss:0.12647846857951014\n",
      "train loss:0.19830519769121963\n",
      "train loss:0.1327377663467037\n",
      "train loss:0.19654196330867663\n",
      "train loss:0.09924463579084913\n",
      "train loss:0.26237867361259004\n",
      "train loss:0.19482949687426107\n",
      "train loss:0.18307327500382087\n",
      "train loss:0.08061426465878972\n",
      "train loss:0.12229627920692833\n",
      "train loss:0.13241318351395642\n",
      "train loss:0.1621545966570763\n",
      "train loss:0.1724137117349451\n",
      "train loss:0.3599505756015324\n",
      "train loss:0.1589975273602543\n",
      "train loss:0.13105950669323593\n",
      "train loss:0.09002917405649402\n",
      "train loss:0.1993585352140292\n",
      "train loss:0.13336773547003164\n",
      "train loss:0.10693476492098462\n",
      "train loss:0.23349436233912765\n",
      "train loss:0.11859115022755123\n",
      "train loss:0.11634874817624093\n",
      "train loss:0.1573293579685419\n",
      "train loss:0.3189823049507634\n",
      "train loss:0.26628236198667454\n",
      "train loss:0.0716760587461603\n",
      "train loss:0.1401709557239702\n",
      "train loss:0.1413410732330123\n",
      "train loss:0.10921575505025795\n",
      "train loss:0.16745362838226477\n",
      "train loss:0.21454289552900005\n",
      "train loss:0.18960540164757997\n",
      "train loss:0.07363793528189354\n",
      "train loss:0.13258626771496568\n",
      "train loss:0.15326660234625444\n",
      "train loss:0.10485821255576226\n",
      "train loss:0.15239547071694026\n",
      "=== epoch:8, train acc:0.937, test acc:0.916 ===\n",
      "train loss:0.11779649521351032\n",
      "train loss:0.07866660481844162\n",
      "train loss:0.12382983630644522\n",
      "train loss:0.11340759950226216\n",
      "train loss:0.16840992167834645\n",
      "train loss:0.06823677181640547\n",
      "train loss:0.10080310018987956\n",
      "train loss:0.1456328643916722\n",
      "train loss:0.15192876249315676\n",
      "train loss:0.1008465159812347\n",
      "train loss:0.06640102508931021\n",
      "train loss:0.11944498810122046\n",
      "train loss:0.12479457079465807\n",
      "train loss:0.11448367721513181\n",
      "train loss:0.11358427536302627\n",
      "train loss:0.1543482930641093\n",
      "train loss:0.192426242680349\n",
      "train loss:0.16008766287810242\n",
      "train loss:0.16505126219220473\n",
      "train loss:0.1274987176432708\n",
      "train loss:0.061684170929810274\n",
      "train loss:0.20326450761219184\n",
      "train loss:0.05569523323854929\n",
      "train loss:0.11877927497640915\n",
      "train loss:0.22654877480679247\n",
      "train loss:0.07438170666983913\n",
      "train loss:0.13092883290561041\n",
      "train loss:0.21856169477386497\n",
      "train loss:0.1725380066941848\n",
      "train loss:0.08184585658058355\n",
      "train loss:0.09299002209957202\n",
      "train loss:0.11833153717900338\n",
      "train loss:0.25417909044635445\n",
      "train loss:0.08936437572134319\n",
      "train loss:0.15744121186872106\n",
      "train loss:0.20234624401144777\n",
      "train loss:0.16379473036564376\n",
      "train loss:0.16851815741435502\n",
      "train loss:0.10242730622051367\n",
      "train loss:0.10465848088034659\n",
      "train loss:0.09724525902052797\n",
      "train loss:0.20713764105223234\n",
      "train loss:0.23768260361700608\n",
      "train loss:0.0970399351503295\n",
      "train loss:0.084512831406106\n",
      "train loss:0.13212765594430195\n",
      "train loss:0.0694375902978646\n",
      "train loss:0.18512872195447325\n",
      "train loss:0.2105545744546569\n",
      "train loss:0.1471265050191354\n",
      "=== epoch:9, train acc:0.948, test acc:0.917 ===\n",
      "train loss:0.0557143954606838\n",
      "train loss:0.21833719535392102\n",
      "train loss:0.14561197105096593\n",
      "train loss:0.15675617279428855\n",
      "train loss:0.10860965037476772\n",
      "train loss:0.16235335944963628\n",
      "train loss:0.10442738658375605\n",
      "train loss:0.13534318940663306\n",
      "train loss:0.12656829366127786\n",
      "train loss:0.07298977530410121\n",
      "train loss:0.08791130364145497\n",
      "train loss:0.08291238528315471\n",
      "train loss:0.08492540813054533\n",
      "train loss:0.09224687595111693\n",
      "train loss:0.09043199637385736\n",
      "train loss:0.11367912721566925\n",
      "train loss:0.09181914492428436\n",
      "train loss:0.08021628845601224\n",
      "train loss:0.11284156435130727\n",
      "train loss:0.07617560245696187\n",
      "train loss:0.1924177908041014\n",
      "train loss:0.14079675945446588\n",
      "train loss:0.3271801754193173\n",
      "train loss:0.17686058524666234\n",
      "train loss:0.18825574321880015\n",
      "train loss:0.13994618004373696\n",
      "train loss:0.1245505974034338\n",
      "train loss:0.16511399776175686\n",
      "train loss:0.0818714390002152\n",
      "train loss:0.18571040767778166\n",
      "train loss:0.1210064488837557\n",
      "train loss:0.18635970206075886\n",
      "train loss:0.0702360452299964\n",
      "train loss:0.08690869892921031\n",
      "train loss:0.12381653077420797\n",
      "train loss:0.08305361580223378\n",
      "train loss:0.035107949956777865\n",
      "train loss:0.12329718393581478\n",
      "train loss:0.11814678002495445\n",
      "train loss:0.12031129602254328\n",
      "train loss:0.10885003250908516\n",
      "train loss:0.13226666376802304\n",
      "train loss:0.14382515393304618\n",
      "train loss:0.08396648560805095\n",
      "train loss:0.047571502946484795\n",
      "train loss:0.182818999500665\n",
      "train loss:0.06610310108852313\n",
      "train loss:0.07566572043675361\n",
      "train loss:0.13796824760023965\n",
      "train loss:0.12030943001197994\n",
      "=== epoch:10, train acc:0.957, test acc:0.931 ===\n",
      "train loss:0.09157747786542797\n",
      "train loss:0.05343231374820791\n",
      "train loss:0.12298436672607846\n",
      "train loss:0.06870079606319669\n",
      "train loss:0.07825376626962947\n",
      "train loss:0.2169035467257368\n",
      "train loss:0.0360179261152625\n",
      "train loss:0.08572491325314058\n",
      "train loss:0.06941908971455041\n",
      "train loss:0.09417681931746386\n",
      "train loss:0.13797312069736234\n",
      "train loss:0.08535695408866723\n",
      "train loss:0.20636987172732554\n",
      "train loss:0.0933310001711461\n",
      "train loss:0.05431963938271484\n",
      "train loss:0.0517413089004079\n",
      "train loss:0.08657875629028851\n",
      "train loss:0.14696195323504008\n",
      "train loss:0.18349565461498063\n",
      "train loss:0.16088463005622383\n",
      "train loss:0.19541346538021792\n",
      "train loss:0.08217419950980515\n",
      "train loss:0.07047721819553032\n",
      "train loss:0.14997408057534126\n",
      "train loss:0.12381261920242742\n",
      "train loss:0.07599792912594436\n",
      "train loss:0.14100639801921308\n",
      "train loss:0.07524526785267827\n",
      "train loss:0.09424477375206233\n",
      "train loss:0.061012599504750645\n",
      "train loss:0.06708172514270895\n",
      "train loss:0.06369627912733603\n",
      "train loss:0.10768322342828181\n",
      "train loss:0.1167821243009042\n",
      "train loss:0.12405405564976801\n",
      "train loss:0.06859996615680412\n",
      "train loss:0.06068597468921795\n",
      "train loss:0.1006487318109084\n",
      "train loss:0.05299432062969764\n",
      "train loss:0.1411305201315905\n",
      "train loss:0.1235885043885458\n",
      "train loss:0.08967676868023505\n",
      "train loss:0.05720048416670285\n",
      "train loss:0.0739126787473741\n",
      "train loss:0.12594040841312207\n",
      "train loss:0.05382723344854821\n",
      "train loss:0.18606635123345897\n",
      "train loss:0.049032242009965185\n",
      "train loss:0.07007329209758058\n",
      "train loss:0.0851771496566365\n",
      "=== epoch:11, train acc:0.961, test acc:0.926 ===\n",
      "train loss:0.13324180166379768\n",
      "train loss:0.17926248128237007\n",
      "train loss:0.11092517158054345\n",
      "train loss:0.08015237290052203\n",
      "train loss:0.07186142996866882\n",
      "train loss:0.05082689747318689\n",
      "train loss:0.12777357624819982\n",
      "train loss:0.05948115601170176\n",
      "train loss:0.06929354296024196\n",
      "train loss:0.10417893319998237\n",
      "train loss:0.05942553558928129\n",
      "train loss:0.2084121675812346\n",
      "train loss:0.15311847733096207\n",
      "train loss:0.16030830831038614\n",
      "train loss:0.14207103716015923\n",
      "train loss:0.15894429418194797\n",
      "train loss:0.14551615337118495\n",
      "train loss:0.10749089274190382\n",
      "train loss:0.055660937075705685\n",
      "train loss:0.1003775691223303\n",
      "train loss:0.09290383911419499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1505823017506741\n",
      "train loss:0.09495162816613244\n",
      "train loss:0.11988693557383576\n",
      "train loss:0.04765132596192742\n",
      "train loss:0.049167901953882646\n",
      "train loss:0.08061730655639829\n",
      "train loss:0.05563844997436563\n",
      "train loss:0.06608634638040316\n",
      "train loss:0.07633167520826425\n",
      "train loss:0.06752075309650694\n",
      "train loss:0.04552061073744469\n",
      "train loss:0.08241558879133035\n",
      "train loss:0.06088708870867337\n",
      "train loss:0.05063502284075899\n",
      "train loss:0.04129191199738927\n",
      "train loss:0.04501419509025794\n",
      "train loss:0.08351693249672237\n",
      "train loss:0.05427107765823552\n",
      "train loss:0.1097859015382937\n",
      "train loss:0.04927913862105207\n",
      "train loss:0.06504314357248162\n",
      "train loss:0.08220362196710619\n",
      "train loss:0.08054341220974401\n",
      "train loss:0.03733568735544391\n",
      "train loss:0.06497972245497025\n",
      "train loss:0.1876353873618756\n",
      "train loss:0.14366235703129296\n",
      "train loss:0.07854706400147217\n",
      "train loss:0.11766326678049577\n",
      "=== epoch:12, train acc:0.964, test acc:0.936 ===\n",
      "train loss:0.07687040973489342\n",
      "train loss:0.0857586186018898\n",
      "train loss:0.14107764780738782\n",
      "train loss:0.03980824278550357\n",
      "train loss:0.031435198774926544\n",
      "train loss:0.06947355949624116\n",
      "train loss:0.06245372154257251\n",
      "train loss:0.051977048830969944\n",
      "train loss:0.04751115776298088\n",
      "train loss:0.05878515281523383\n",
      "train loss:0.0749768540815985\n",
      "train loss:0.06204302056595373\n",
      "train loss:0.10566698912038247\n",
      "train loss:0.06970133298779319\n",
      "train loss:0.035044558726795356\n",
      "train loss:0.11372789953098389\n",
      "train loss:0.04671841443041935\n",
      "train loss:0.05241150446028089\n",
      "train loss:0.09273964383236112\n",
      "train loss:0.06271999878812232\n",
      "train loss:0.057439842295064006\n",
      "train loss:0.08876664298514393\n",
      "train loss:0.04870787969102503\n",
      "train loss:0.0335012819153004\n",
      "train loss:0.07001717988773447\n",
      "train loss:0.16127740020075\n",
      "train loss:0.12264030530818452\n",
      "train loss:0.14307847908254273\n",
      "train loss:0.02694484643687169\n",
      "train loss:0.026872165271009437\n",
      "train loss:0.11362499693672562\n",
      "train loss:0.05306962948103498\n",
      "train loss:0.11187289928696022\n",
      "train loss:0.09720832425692595\n",
      "train loss:0.10864759300140595\n",
      "train loss:0.10412099808049755\n",
      "train loss:0.06776062754750375\n",
      "train loss:0.041028779366439665\n",
      "train loss:0.06260092391367704\n",
      "train loss:0.16232171060095119\n",
      "train loss:0.12680099363418956\n",
      "train loss:0.11134067319590367\n",
      "train loss:0.08443788322544174\n",
      "train loss:0.045317546848342875\n",
      "train loss:0.07243965254877616\n",
      "train loss:0.07120149853628745\n",
      "train loss:0.12040302238155394\n",
      "train loss:0.16074088590538588\n",
      "train loss:0.03933855854722779\n",
      "train loss:0.029989466758340196\n",
      "=== epoch:13, train acc:0.965, test acc:0.941 ===\n",
      "train loss:0.06281125628543821\n",
      "train loss:0.04727086551975164\n",
      "train loss:0.07098183305655707\n",
      "train loss:0.06038434084401606\n",
      "train loss:0.05441377278806288\n",
      "train loss:0.08982323802178825\n",
      "train loss:0.07045973684884929\n",
      "train loss:0.06078873682325131\n",
      "train loss:0.04171914872220356\n",
      "train loss:0.0866448972707221\n",
      "train loss:0.06812414138810247\n",
      "train loss:0.08166010648750412\n",
      "train loss:0.026287816947742012\n",
      "train loss:0.0723264776551593\n",
      "train loss:0.04697843482222344\n",
      "train loss:0.06183594011650258\n",
      "train loss:0.028755062399097947\n",
      "train loss:0.13506185112025096\n",
      "train loss:0.028051468711336586\n",
      "train loss:0.05659642768375235\n",
      "train loss:0.03736022295645069\n",
      "train loss:0.15950427417155108\n",
      "train loss:0.09560727965579703\n",
      "train loss:0.10819698991962272\n",
      "train loss:0.08118432746355465\n",
      "train loss:0.07319032416416989\n",
      "train loss:0.11661468897631153\n",
      "train loss:0.06879599796412127\n",
      "train loss:0.0677022223257044\n",
      "train loss:0.09144179891007853\n",
      "train loss:0.03377663420966551\n",
      "train loss:0.05497819458542686\n",
      "train loss:0.07179900230472382\n",
      "train loss:0.06253666972227138\n",
      "train loss:0.031664401195274694\n",
      "train loss:0.052659070833055634\n",
      "train loss:0.1338814158182515\n",
      "train loss:0.044552899898240156\n",
      "train loss:0.10058968354532162\n",
      "train loss:0.07956432040367804\n",
      "train loss:0.04231020440214826\n",
      "train loss:0.0594806798169639\n",
      "train loss:0.09319785929428648\n",
      "train loss:0.06322777215775675\n",
      "train loss:0.03198998809761493\n",
      "train loss:0.04250113510786421\n",
      "train loss:0.07319996362400903\n",
      "train loss:0.07436501073892819\n",
      "train loss:0.06805569662678047\n",
      "train loss:0.03453628132146734\n",
      "=== epoch:14, train acc:0.974, test acc:0.94 ===\n",
      "train loss:0.03095162475142298\n",
      "train loss:0.040007352723470314\n",
      "train loss:0.041682935748246025\n",
      "train loss:0.07992742064125467\n",
      "train loss:0.0782989549920262\n",
      "train loss:0.05008421792005553\n",
      "train loss:0.08121013741051045\n",
      "train loss:0.05204164195562684\n",
      "train loss:0.13860946515471217\n",
      "train loss:0.14355131219898604\n",
      "train loss:0.07750652992983419\n",
      "train loss:0.14074374204296192\n",
      "train loss:0.02306480311951268\n",
      "train loss:0.03841607416110868\n",
      "train loss:0.08012990595949955\n",
      "train loss:0.06709681866775119\n",
      "train loss:0.08036455494982192\n",
      "train loss:0.030291312745397113\n",
      "train loss:0.061728777865281886\n",
      "train loss:0.059283269427574246\n",
      "train loss:0.048956371660295775\n",
      "train loss:0.03986642091900247\n",
      "train loss:0.033140390283166045\n",
      "train loss:0.07760242873521389\n",
      "train loss:0.09236633576899016\n",
      "train loss:0.0465179173302773\n",
      "train loss:0.07191800956854338\n",
      "train loss:0.07876054304589494\n",
      "train loss:0.03735158978108832\n",
      "train loss:0.05529045764064931\n",
      "train loss:0.016873449854328332\n",
      "train loss:0.04407615414726595\n",
      "train loss:0.03963146692606665\n",
      "train loss:0.0823087425877517\n",
      "train loss:0.07677775702584405\n",
      "train loss:0.028876063858071545\n",
      "train loss:0.07965209060773551\n",
      "train loss:0.08976590302361671\n",
      "train loss:0.10298227734466735\n",
      "train loss:0.04892909670706043\n",
      "train loss:0.06181786190600441\n",
      "train loss:0.03530213341060031\n",
      "train loss:0.031507396256746485\n",
      "train loss:0.08896134889638393\n",
      "train loss:0.025275643737653092\n",
      "train loss:0.15265071863659652\n",
      "train loss:0.03238108661285823\n",
      "train loss:0.11388025018739857\n",
      "train loss:0.10385136893231393\n",
      "train loss:0.0435782185194173\n",
      "=== epoch:15, train acc:0.969, test acc:0.931 ===\n",
      "train loss:0.08072771655160813\n",
      "train loss:0.05000952374031518\n",
      "train loss:0.0710273085435691\n",
      "train loss:0.07715488960165208\n",
      "train loss:0.08046598512877999\n",
      "train loss:0.1490276978960744\n",
      "train loss:0.017943864261031547\n",
      "train loss:0.048360931404563566\n",
      "train loss:0.04895063673150009\n",
      "train loss:0.04279602377744181\n",
      "train loss:0.04941169483268177\n",
      "train loss:0.030817133270344754\n",
      "train loss:0.049234972444007846\n",
      "train loss:0.08000985379012125\n",
      "train loss:0.07160856774926061\n",
      "train loss:0.013363997640352432\n",
      "train loss:0.02521476625657812\n",
      "train loss:0.0470551759897399\n",
      "train loss:0.07763381566434982\n",
      "train loss:0.06441316592412288\n",
      "train loss:0.0381750864110674\n",
      "train loss:0.08214377018625257\n",
      "train loss:0.05330760388071469\n",
      "train loss:0.07435662087927616\n",
      "train loss:0.02427571537481055\n",
      "train loss:0.05679106961073062\n",
      "train loss:0.06216389242501537\n",
      "train loss:0.03304139058127061\n",
      "train loss:0.02686225078923408\n",
      "train loss:0.05804369152404749\n",
      "train loss:0.04772633902972896\n",
      "train loss:0.041971819802788496\n",
      "train loss:0.04336003681015391\n",
      "train loss:0.04576056532863603\n",
      "train loss:0.03926121169987712\n",
      "train loss:0.043187289013812774\n",
      "train loss:0.07488553915547196\n",
      "train loss:0.06054679391060977\n",
      "train loss:0.06285354377071589\n",
      "train loss:0.05271273901949016\n",
      "train loss:0.10470631837230225\n",
      "train loss:0.03304406248668677\n",
      "train loss:0.07286814218931148\n",
      "train loss:0.0463117123156094\n",
      "train loss:0.049991860563506664\n",
      "train loss:0.05201127754084997\n",
      "train loss:0.047532813320982116\n",
      "train loss:0.06053457817202058\n",
      "train loss:0.06111742469058739\n",
      "train loss:0.04030675832099886\n",
      "=== epoch:16, train acc:0.978, test acc:0.95 ===\n",
      "train loss:0.014595175533543241\n",
      "train loss:0.01699485623808887\n",
      "train loss:0.03284560445035799\n",
      "train loss:0.03571077475471073\n",
      "train loss:0.03183094879434348\n",
      "train loss:0.026065652038670076\n",
      "train loss:0.059281524117061044\n",
      "train loss:0.10562948033149372\n",
      "train loss:0.010256448085432296\n",
      "train loss:0.03898415609212537\n",
      "train loss:0.05371874588722071\n",
      "train loss:0.04644991758234267\n",
      "train loss:0.02848121375199976\n",
      "train loss:0.030525339163876532\n",
      "train loss:0.04525850543833955\n",
      "train loss:0.012882509201654362\n",
      "train loss:0.04445698783396318\n",
      "train loss:0.02699558830141947\n",
      "train loss:0.04717989472900916\n",
      "train loss:0.019990268249684015\n",
      "train loss:0.054948457218531026\n",
      "train loss:0.045974360022391615\n",
      "train loss:0.05963111844222168\n",
      "train loss:0.05171184021674977\n",
      "train loss:0.044449541872206656\n",
      "train loss:0.05499953202717509\n",
      "train loss:0.015531973955497179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.012137142550927415\n",
      "train loss:0.05832814653229249\n",
      "train loss:0.05612533729655275\n",
      "train loss:0.029242092000854578\n",
      "train loss:0.05080375002147115\n",
      "train loss:0.031155463145247385\n",
      "train loss:0.013468440435360475\n",
      "train loss:0.03556069660454315\n",
      "train loss:0.045186415503281646\n",
      "train loss:0.035705197012604814\n",
      "train loss:0.014935666278199849\n",
      "train loss:0.0407611810034923\n",
      "train loss:0.012727142840321797\n",
      "train loss:0.04182867188650545\n",
      "train loss:0.04422577076827915\n",
      "train loss:0.03139474278183691\n",
      "train loss:0.03344127664775933\n",
      "train loss:0.016782670978133317\n",
      "train loss:0.0578609373810523\n",
      "train loss:0.12338734322534009\n",
      "train loss:0.1111286849099218\n",
      "train loss:0.04481082008121966\n",
      "train loss:0.03041954172406449\n",
      "=== epoch:17, train acc:0.982, test acc:0.944 ===\n",
      "train loss:0.03654818328950835\n",
      "train loss:0.03028379093105247\n",
      "train loss:0.028163219273442723\n",
      "train loss:0.031433059210660204\n",
      "train loss:0.03283495291912375\n",
      "train loss:0.01927394770647017\n",
      "train loss:0.06103552434327056\n",
      "train loss:0.03164230034665962\n",
      "train loss:0.0877977281380909\n",
      "train loss:0.06796688688615939\n",
      "train loss:0.032541529365083345\n",
      "train loss:0.02766801512634416\n",
      "train loss:0.09243251431641006\n",
      "train loss:0.014635409686946075\n",
      "train loss:0.03886306354985364\n",
      "train loss:0.058637010935584495\n",
      "train loss:0.028533880127924734\n",
      "train loss:0.061869653333023786\n",
      "train loss:0.028147439582666696\n",
      "train loss:0.024829948067103785\n",
      "train loss:0.07084203911745572\n",
      "train loss:0.027852680687475213\n",
      "train loss:0.03200788612580709\n",
      "train loss:0.014598781123964732\n",
      "train loss:0.017413043094106215\n",
      "train loss:0.07011011203356594\n",
      "train loss:0.05600026126860895\n",
      "train loss:0.047268576048297636\n",
      "train loss:0.06273648288449539\n",
      "train loss:0.012988842725306664\n",
      "train loss:0.059787846229925744\n",
      "train loss:0.10145874792634708\n",
      "train loss:0.07535802208518422\n",
      "train loss:0.02998628713571329\n",
      "train loss:0.019679430981273574\n",
      "train loss:0.012736698816810823\n",
      "train loss:0.03957061228142225\n",
      "train loss:0.030664452065066586\n",
      "train loss:0.02764191883606639\n",
      "train loss:0.07245506026519746\n",
      "train loss:0.03911282397199008\n",
      "train loss:0.014843109513861443\n",
      "train loss:0.032912647833461685\n",
      "train loss:0.03359187091559621\n",
      "train loss:0.030163778638894106\n",
      "train loss:0.04147546547051921\n",
      "train loss:0.028892797034067653\n",
      "train loss:0.037770385452923064\n",
      "train loss:0.03775083967100808\n",
      "train loss:0.026525944270943572\n",
      "=== epoch:18, train acc:0.99, test acc:0.947 ===\n",
      "train loss:0.04122338354896194\n",
      "train loss:0.036098800236506606\n",
      "train loss:0.010292043957746862\n",
      "train loss:0.04578995799546443\n",
      "train loss:0.02522434927829737\n",
      "train loss:0.015390978042621167\n",
      "train loss:0.017532269806158466\n",
      "train loss:0.018299953849952616\n",
      "train loss:0.016568924477276193\n",
      "train loss:0.02886538829132546\n",
      "train loss:0.01559153071943205\n",
      "train loss:0.034391073945809564\n",
      "train loss:0.020386309126911768\n",
      "train loss:0.010854680038869038\n",
      "train loss:0.02338289336353018\n",
      "train loss:0.0341154504138486\n",
      "train loss:0.04212603686521211\n",
      "train loss:0.028238463268182117\n",
      "train loss:0.017718141127369906\n",
      "train loss:0.02902346162604275\n",
      "train loss:0.05701688148308506\n",
      "train loss:0.03276738407325087\n",
      "train loss:0.03907255852338057\n",
      "train loss:0.025998576572937682\n",
      "train loss:0.06898996833727034\n",
      "train loss:0.06064493178451033\n",
      "train loss:0.015354331439018905\n",
      "train loss:0.01736245266288941\n",
      "train loss:0.058281881128481004\n",
      "train loss:0.03382715171104629\n",
      "train loss:0.035029538472021275\n",
      "train loss:0.013933688536726427\n",
      "train loss:0.036148438602786784\n",
      "train loss:0.020727546116645824\n",
      "train loss:0.02418276057292595\n",
      "train loss:0.009748168011325178\n",
      "train loss:0.03869034294712683\n",
      "train loss:0.054710580081943586\n",
      "train loss:0.016415540980048562\n",
      "train loss:0.05247091654550288\n",
      "train loss:0.012578759091026208\n",
      "train loss:0.06368591130422555\n",
      "train loss:0.0420104788699785\n",
      "train loss:0.03828854355609148\n",
      "train loss:0.02217800583930872\n",
      "train loss:0.03000932399275931\n",
      "train loss:0.06659497721737945\n",
      "train loss:0.026732622404181325\n",
      "train loss:0.05182205423009485\n",
      "train loss:0.01990339498290456\n",
      "=== epoch:19, train acc:0.983, test acc:0.944 ===\n",
      "train loss:0.10066791312050884\n",
      "train loss:0.015705843517723195\n",
      "train loss:0.018206917165902958\n",
      "train loss:0.013858453103697977\n",
      "train loss:0.030998026267904027\n",
      "train loss:0.04327739031158112\n",
      "train loss:0.017827595097112018\n",
      "train loss:0.007898303802511305\n",
      "train loss:0.027362280539029168\n",
      "train loss:0.028618951461499304\n",
      "train loss:0.022614031276821964\n",
      "train loss:0.0408347152982823\n",
      "train loss:0.08421487731583782\n",
      "train loss:0.017434114955544172\n",
      "train loss:0.04218831536467236\n",
      "train loss:0.03566185578981301\n",
      "train loss:0.02385696565592351\n",
      "train loss:0.0290704102968169\n",
      "train loss:0.02502780778386897\n",
      "train loss:0.027778580555662354\n",
      "train loss:0.027281290694767268\n",
      "train loss:0.01697623516552791\n",
      "train loss:0.04352898722113254\n",
      "train loss:0.01265569497045603\n",
      "train loss:0.02675350768689958\n",
      "train loss:0.018875141489921955\n",
      "train loss:0.00722180551696611\n",
      "train loss:0.023608822913421926\n",
      "train loss:0.05806012336112188\n",
      "train loss:0.009710567779505422\n",
      "train loss:0.06880405887970391\n",
      "train loss:0.023761275463053906\n",
      "train loss:0.033780867161546484\n",
      "train loss:0.027919374352754454\n",
      "train loss:0.026975595610095094\n",
      "train loss:0.03293244677885522\n",
      "train loss:0.01843318921777102\n",
      "train loss:0.026189705129758382\n",
      "train loss:0.023372626190436147\n",
      "train loss:0.02592675682544513\n",
      "train loss:0.012351394278127226\n",
      "train loss:0.013649848422083166\n",
      "train loss:0.013759126220216206\n",
      "train loss:0.05676212459073442\n",
      "train loss:0.020704736687456822\n",
      "train loss:0.031144513381075524\n",
      "train loss:0.02670550191112634\n",
      "train loss:0.011807920196447535\n",
      "train loss:0.018085335947596967\n",
      "train loss:0.019126145111579326\n",
      "=== epoch:20, train acc:0.992, test acc:0.951 ===\n",
      "train loss:0.010084075130508484\n",
      "train loss:0.046326625176824315\n",
      "train loss:0.03906543554288711\n",
      "train loss:0.05357761056618051\n",
      "train loss:0.01638155473457118\n",
      "train loss:0.009665516175546245\n",
      "train loss:0.008667432308614454\n",
      "train loss:0.03495484449533058\n",
      "train loss:0.018737090462259475\n",
      "train loss:0.02224755836903146\n",
      "train loss:0.052661370986955586\n",
      "train loss:0.017753090530781616\n",
      "train loss:0.012932895415522736\n",
      "train loss:0.026797508185651207\n",
      "train loss:0.033603905584799885\n",
      "train loss:0.038911019795857774\n",
      "train loss:0.02428372268355797\n",
      "train loss:0.027676783785476233\n",
      "train loss:0.010788764329293437\n",
      "train loss:0.015108672411108224\n",
      "train loss:0.03626976943565591\n",
      "train loss:0.018747621494333958\n",
      "train loss:0.03396599636436545\n",
      "train loss:0.03156006276815372\n",
      "train loss:0.042472346619579195\n",
      "train loss:0.025170420087881746\n",
      "train loss:0.02056415942947517\n",
      "train loss:0.022578565511280093\n",
      "train loss:0.02357534749408363\n",
      "train loss:0.017120463278938992\n",
      "train loss:0.00727357778189174\n",
      "train loss:0.020313904852273013\n",
      "train loss:0.010078613959666194\n",
      "train loss:0.01054261587475545\n",
      "train loss:0.0061092466727293415\n",
      "train loss:0.02066282739320588\n",
      "train loss:0.02830374556958575\n",
      "train loss:0.029066080405768767\n",
      "train loss:0.09461294657711332\n",
      "train loss:0.008838030231244907\n",
      "train loss:0.01601692619677394\n",
      "train loss:0.02298395260996338\n",
      "train loss:0.019099058576040006\n",
      "train loss:0.04337274918389178\n",
      "train loss:0.030224449500295122\n",
      "train loss:0.01501738004461877\n",
      "train loss:0.008572740704904968\n",
      "train loss:0.025462490708920083\n",
      "train loss:0.01563465531944954\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.952\n",
      "Save Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(\n",
    "    input_dim=(1, 28, 28),\n",
    "    conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "    hidden_size=100,\n",
    "    output_size=10,\n",
    "    weight_init_std=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    network, x_train, t_train, x_test, t_test, epochs=max_epochs, mini_batch_size=100, optimizer='Adam',\n",
    "    optimizer_param={'lr': 0.001}, evaluate_sample_num_per_epoch=1000\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "network.save_params('params.pkl')\n",
    "print('Save Network Parameters!')\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36694.244163,
   "end_time": "2022-06-01T15:52:45.852922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-01T05:41:11.608759",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
