{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO/jgReOorG2fC1bRMV3dew"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 7-4　BERT によるマルチラベル分類"],"metadata":{"id":"r-JhMCLL0kII"}},{"cell_type":"code","source":["# !mkdir chap7\n","%cd ./chap7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPiO0XOnf4wz","executionInfo":{"status":"ok","timestamp":1716535851873,"user_tz":-540,"elapsed":5,"user":{"displayName":"zoo","userId":"06961267499399105182"}},"outputId":"dfb9805f-d5ee-40ae-f2bc-bd419b560291"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/chap7\n"]}]},{"cell_type":"code","source":["# !pip install transformers==4.18.0 fugashi===1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1"],"metadata":{"id":"_8ufrt-20fef","executionInfo":{"status":"ok","timestamp":1716535851874,"user_tz":-540,"elapsed":4,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import glob\n","import json\n","import torch\n","import random\n","import numpy as np\n","import pytorch_lightning as pl\n","\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertModel"],"metadata":{"id":"FDcnwwQn0Bo_","executionInfo":{"status":"ok","timestamp":1716535859180,"user_tz":-540,"elapsed":7310,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["MODEL_NAME = 'tohoku-nlp/bert-base-japanese-whole-word-masking'"],"metadata":{"id":"UwuK523_g0pk","executionInfo":{"status":"ok","timestamp":1716535859180,"user_tz":-540,"elapsed":4,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n","\n","  def __init__(self, model_name, num_labels):\n","    super().__init__()\n","    self.bert = BertModel.from_pretrained(model_name)\n","    self.linear = torch.nn.Linear(self.bert.config.hidden_size, num_labels)\n","\n","  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n","    bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","    last_hidden_state = bert_output.last_hidden_state\n","\n","    averaged_hidden_state = (last_hidden_state*attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n","    scores = self.linear(averaged_hidden_state)\n","    output = {'logits': scores}\n","\n","    if labels is not None:\n","      loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n","      output['loss'] = loss\n","\n","    output = type('bert_output', (object,), output)\n","\n","    return output"],"metadata":{"id":"bajPFGSGvRMa","executionInfo":{"status":"ok","timestamp":1716535859180,"user_tz":-540,"elapsed":4,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["masked_hidden_state = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n","                                    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]])\n","print(masked_hidden_state.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BW9AVHTw936","executionInfo":{"status":"ok","timestamp":1716535859180,"user_tz":-540,"elapsed":4,"user":{"displayName":"zoo","userId":"06961267499399105182"}},"outputId":"3ef6416b-0ee9-4503-ce2f-f000750bae9b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 4])\n"]}]},{"cell_type":"code","source":["sum_hidden_state = masked_hidden_state.sum(1)\n","print(sum_hidden_state)\n","print(sum_hidden_state.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3PbMKAZ2kSg","executionInfo":{"status":"ok","timestamp":1716535859180,"user_tz":-540,"elapsed":3,"user":{"displayName":"zoo","userId":"06961267499399105182"}},"outputId":"8fca35b9-caab-4012-d389-a4c4e4ed56a9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[15, 18, 21, 24],\n","        [51, 54, 57, 60]])\n","torch.Size([2, 4])\n"]}]},{"cell_type":"code","source":["tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","bert_scml = BertForSequenceClassificationMultiLabel(MODEL_NAME, num_labels=2)\n","bert_scml = bert_scml.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8hPRINMB2t4L","executionInfo":{"status":"ok","timestamp":1716535863931,"user_tz":-540,"elapsed":4753,"user":{"displayName":"zoo","userId":"06961267499399105182"}},"outputId":"63eb6828-28ff-49b4-bce2-c56bba9f199e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["text_list = ['今日の仕事はうまくいったが、体調があまり良くない。', '昨日は楽しかった。']\n","labels_list = [[1, 1], [0, 1]]\n","\n","encoding = tokenizer(text_list, padding='longest', return_tensors='pt')\n","encoding = {k: v.cuda() for k, v in encoding.items()}\n","labels = torch.tensor(labels_list).cuda()\n","\n","with torch.no_grad():\n","  output = bert_scml(**encoding)\n","scores = output.logits\n","\n","labels_predicted = (scores > 0).int()\n","\n","num_correct = (labels_predicted == labels).all(-1).sum().item()\n","accuracy = num_correct / labels.size(0)"],"metadata":{"id":"uc_zudsBB6nM","executionInfo":{"status":"ok","timestamp":1716535865256,"user_tz":-540,"elapsed":1328,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["encoding = tokenizer(text_list, padding='longest', return_tensors='pt')\n","encoding['labels'] = torch.tensor(labels_list)\n","encoding = {k: v.cuda() for k, v in encoding.items()}\n","\n","output = bert_scml(**encoding)\n","loss = output.loss"],"metadata":{"id":"1M6Hc5H5f2k_","executionInfo":{"status":"ok","timestamp":1716535865256,"user_tz":-540,"elapsed":3,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":10,"outputs":[]}]}