{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO63p/jgp1dnugnB3cqQbUr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 6-3　BERT による文章分類"],"metadata":{"id":"r-JhMCLL0kII"}},{"cell_type":"code","source":["# !mkdir chap6\n","%cd ./chap6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPiO0XOnf4wz","executionInfo":{"status":"ok","timestamp":1716192604876,"user_tz":-540,"elapsed":4,"user":{"displayName":"zoo","userId":"06961267499399105182"}},"outputId":"40552018-2eb8-4074-8539-fcf4d3d2a518"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/chap6\n"]}]},{"cell_type":"code","source":["# !pip install transformers==4.18.0 fugashi===1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1"],"metadata":{"id":"_8ufrt-20fef","executionInfo":{"status":"ok","timestamp":1716192604877,"user_tz":-540,"elapsed":3,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import glob\n","import torch\n","import random\n","import numpy as np\n","import pytorch_lightning as pl\n","\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForSequenceClassification"],"metadata":{"id":"FDcnwwQn0Bo_","executionInfo":{"status":"ok","timestamp":1716192615523,"user_tz":-540,"elapsed":10649,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["MODEL_NAME = 'tohoku-nlp/bert-base-japanese-whole-word-masking'"],"metadata":{"id":"UwuK523_g0pk","executionInfo":{"status":"ok","timestamp":1716192615523,"user_tz":-540,"elapsed":3,"user":{"displayName":"zoo","userId":"06961267499399105182"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","bert_sc = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n","bert_sc = bert_sc.cuda()"],"metadata":{"id":"elSL5xz8iLqf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716192618339,"user_tz":-540,"elapsed":2818,"user":{"displayName":"zoo","userId":"06961267499399105182"}},"outputId":"e6444419-bb46-431f-c6ed-479c1cd449bf"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["text_list = [\n","    'この映画は面白かった。',\n","    'この映画の最後にはがっかりさせられた。',\n","    'この映画を見て幸せな気持ちになった。'\n","    ]\n","\n","label_list = [1, 0, 1]\n","\n","encoding = tokenizer(text_list, padding='longest', return_tensors='pt')\n","encoding = {k: v.cuda() for k, v in encoding.items()}\n","labels = torch.tensor(label_list).cuda()\n","\n","with torch.no_grad():\n","  output = bert_sc.forward(**encoding)\n","score = output.logits\n","labels_predicted = score.argmax(-1)\n","num_correct = (labels_predicted==labels).sum().item()\n","accuracy = num_correct / labels.size(0)\n","\n","print('# scores:')\n","print(score.size())\n","print('# predicted labels:')\n","print(labels_predicted)\n","print('# accuracy')\n","print(accuracy)"],"metadata":{"id":"vgolQkbDiLsz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716192618695,"user_tz":-540,"elapsed":376,"user":{"displayName":"zoo","userId":"06961267499399105182"}},"outputId":"8985d748-bf2a-49f7-8019-322ba8a91dc5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["# scores:\n","torch.Size([3, 2])\n","# predicted labels:\n","tensor([1, 1, 1], device='cuda:0')\n","# accuracy\n","0.6666666666666666\n"]}]},{"cell_type":"code","source":["encoding = tokenizer(text_list, padding='longest', return_tensors='pt')\n","encoding['labels'] = torch.tensor(label_list)\n","encoding = {k: v.cuda() for k, v in encoding.items()}\n","\n","output = bert_sc(**encoding)\n","loss = output.loss\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CcIddfYAK4f","executionInfo":{"status":"ok","timestamp":1716192619083,"user_tz":-540,"elapsed":390,"user":{"displayName":"zoo","userId":"06961267499399105182"}},"outputId":"f005d038-daf6-4c65-f8ca-ce3388b2fe9e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]}]}]}